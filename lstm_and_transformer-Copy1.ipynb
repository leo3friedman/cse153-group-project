{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70390049-152d-4f80-82c5-813f99ce6e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /venv/main/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebc366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913d95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, prefix='', length=50):\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = '█' * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% Complete', end='\\r', flush=True)\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5a311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_midi_files(filepaths):\n",
    "    valid_files = []\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        try:\n",
    "            midi_data = pretty_midi.PrettyMIDI(filepath)\n",
    "            if len(midi_data.instruments) > 0:\n",
    "                valid_files.append(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "        print_progress_bar(i+1, len(filepaths), prefix='Validating MIDI files')\n",
    "    return valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d899ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing nesmdb_midi/train/064_DeepDungeonII_YuushinoMonshou_09_10DungeonFloor3.mid: MIDI file has a largest tick of 59777468, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/090_DragonWarriorIII_30_31IntoTheLegend.mid: MIDI file has a largest tick of 16605640, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/091_DragonWarriorIV_43_44FinaleGuidingPeople.mid: MIDI file has a largest tick of 25710905, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/104_FamicomJumpII_Saikyono7_nin_19_20ThemeofFriendshipEffortVictoryCreditRoll.mid: MIDI file has a largest tick of 12944249, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/117_FinalFantasy_17_18EndTheme.mid: MIDI file has a largest tick of 11324494, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/122_FireEmblem_AnkokuRyutoHikarinoTsurugi_28_29EndingOmnibusBallad.mid: MIDI file has a largest tick of 17014635, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/122_FireEmblem_AnkokuRyutoHikarinoTsurugi_30_31EndingOmnibus.mid: MIDI file has a largest tick of 13007350, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/193_Klax_03_04DanceoftheFairies.mid: MIDI file has a largest tick of 18039578, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/201_LanMaster_01_02Gameplay.mid: MIDI file has a largest tick of 10742782, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/215_Magician_05_06TheLake.mid: MIDI file has a largest tick of 10755932, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/215_Magician_08_09MountVunarCavernsAbadonsCastle.mid: MIDI file has a largest tick of 16907305, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/215_Magician_12_13CorridorofGates.mid: MIDI file has a largest tick of 14794736, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/215_Magician_15_16EpiloguePart1.mid: MIDI file has a largest tick of 24797107, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/215_Magician_14_15AbadonBattleFinalBoss.mid: MIDI file has a largest tick of 11272539, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/229_MegaMan6_35_36EndingOSTEdit.mid: MIDI file has a largest tick of 10348608, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_03_04PreludonGameplay.mid: MIDI file has a largest tick of 28131207, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_09_10MexomorfGameplay.mid: MIDI file has a largest tick of 11175129, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_12_13OmebruGameplay.mid: MIDI file has a largest tick of 66929847, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_14_15CorsoQweroGameplay.mid: MIDI file has a largest tick of 14192721, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_18_19LemonteGameplay.mid: MIDI file has a largest tick of 12682092, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/298_SolarJetman_HuntfortheGoldenWarpship_22_23ShishkebabGameplay.mid: MIDI file has a largest tick of 34300459, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/303_SpaceHarrier_02_03Theme.mid: MIDI file has a largest tick of 16906618, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_02_03JetterStage1FirstHalfArea5.mid: MIDI file has a largest tick of 12523014, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_04_05MOMStage1SecondHalfArea3.mid: MIDI file has a largest tick of 11290582, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_05_06HydeStage2Area6.mid: MIDI file has a largest tick of 13805692, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_08_09TeraArea2.mid: MIDI file has a largest tick of 12542620, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_12_13HienerScoreAttackFirstHalf.mid: MIDI file has a largest tick of 10001656, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/314_SummerCarnival_92_Recca_13_14GelgoogScoreAttackSecondHalf.mid: MIDI file has a largest tick of 10957235, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/329_SwordMaster_04_05MapScreen.mid: MIDI file has a largest tick of 26165568, it is likely corrupt\n",
      "Error processing nesmdb_midi/train/405_ZombieNation_03_04VergeofDangerRoundSelect.mid: MIDI file has a largest tick of 18033915, it is likely corrupt\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "midi_dirpath = 'nesmdb_midi/'\n",
    "midi_train_dirpath = os.path.join(midi_dirpath, 'train')\n",
    "midi_test_dirpath = os.path.join(midi_dirpath, 'test')\n",
    "midi_val_dirpath = os.path.join(midi_dirpath, 'valid')\n",
    "midi_train_filesnames = os.listdir(midi_train_dirpath)\n",
    "midi_test_filesnames = os.listdir(midi_test_dirpath)\n",
    "midi_val_filenames = os.listdir(midi_val_dirpath)\n",
    "\n",
    "midi_train_filepaths = valid_midi_files([os.path.join(midi_train_dirpath, filename) for filename in midi_train_filesnames])\n",
    "midi_test_filepaths = valid_midi_files([os.path.join(midi_test_dirpath, filename) for filename in midi_test_filesnames])\n",
    "midi_val_filepaths = valid_midi_files([os.path.join(midi_val_dirpath, filename) for filename in midi_val_filenames])\n",
    "all_filepaths = midi_train_filepaths + midi_test_filepaths + midi_val_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3098f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1127\n",
      "Vocabulary: {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, 'time_shift_1': 3, 'time_shift_2': 4, 'time_shift_3': 5, 'time_shift_4': 6, 'time_shift_5': 7, 'time_shift_6': 8, 'time_shift_7': 9, 'time_shift_8': 10, 'time_shift_9': 11, 'time_shift_10': 12, 'time_shift_11': 13, 'time_shift_12': 14, 'time_shift_13': 15, 'time_shift_14': 16, 'time_shift_15': 17, 'time_shift_16': 18, 'time_shift_17': 19, 'time_shift_18': 20, 'time_shift_19': 21, 'time_shift_20': 22, 'time_shift_21': 23, 'time_shift_22': 24, 'time_shift_23': 25, 'time_shift_24': 26, 'time_shift_25': 27, 'time_shift_26': 28, 'time_shift_27': 29, 'time_shift_28': 30, 'time_shift_29': 31, 'time_shift_30': 32, 'time_shift_31': 33, 'time_shift_32': 34, 'time_shift_33': 35, 'time_shift_34': 36, 'time_shift_35': 37, 'time_shift_36': 38, 'time_shift_37': 39, 'time_shift_38': 40, 'time_shift_39': 41, 'time_shift_40': 42, 'time_shift_41': 43, 'time_shift_42': 44, 'time_shift_43': 45, 'time_shift_44': 46, 'time_shift_45': 47, 'time_shift_46': 48, 'time_shift_47': 49, 'time_shift_48': 50, 'time_shift_49': 51, 'time_shift_50': 52, 'time_shift_51': 53, 'time_shift_52': 54, 'time_shift_53': 55, 'time_shift_54': 56, 'time_shift_55': 57, 'time_shift_56': 58, 'time_shift_57': 59, 'time_shift_58': 60, 'time_shift_59': 61, 'time_shift_60': 62, 'time_shift_61': 63, 'time_shift_62': 64, 'time_shift_63': 65, 'time_shift_64': 66, 'time_shift_65': 67, 'time_shift_66': 68, 'time_shift_67': 69, 'time_shift_68': 70, 'time_shift_69': 71, 'time_shift_70': 72, 'time_shift_71': 73, 'time_shift_72': 74, 'time_shift_73': 75, 'time_shift_74': 76, 'time_shift_75': 77, 'time_shift_76': 78, 'time_shift_77': 79, 'time_shift_78': 80, 'time_shift_79': 81, 'time_shift_80': 82, 'time_shift_81': 83, 'time_shift_82': 84, 'time_shift_83': 85, 'time_shift_84': 86, 'time_shift_85': 87, 'time_shift_86': 88, 'time_shift_87': 89, 'time_shift_88': 90, 'time_shift_89': 91, 'time_shift_90': 92, 'time_shift_91': 93, 'time_shift_92': 94, 'time_shift_93': 95, 'time_shift_94': 96, 'time_shift_95': 97, 'time_shift_96': 98, 'time_shift_97': 99, 'time_shift_98': 100, 'time_shift_99': 101, 'time_shift_100': 102, 'note_on_0_instrument_80': 103, 'note_on_0_instrument_81': 104, 'note_on_0_instrument_38': 105, 'note_on_0_instrument_121': 106, 'note_on_1_instrument_80': 107, 'note_on_1_instrument_81': 108, 'note_on_1_instrument_38': 109, 'note_on_1_instrument_121': 110, 'note_on_2_instrument_80': 111, 'note_on_2_instrument_81': 112, 'note_on_2_instrument_38': 113, 'note_on_2_instrument_121': 114, 'note_on_3_instrument_80': 115, 'note_on_3_instrument_81': 116, 'note_on_3_instrument_38': 117, 'note_on_3_instrument_121': 118, 'note_on_4_instrument_80': 119, 'note_on_4_instrument_81': 120, 'note_on_4_instrument_38': 121, 'note_on_4_instrument_121': 122, 'note_on_5_instrument_80': 123, 'note_on_5_instrument_81': 124, 'note_on_5_instrument_38': 125, 'note_on_5_instrument_121': 126, 'note_on_6_instrument_80': 127, 'note_on_6_instrument_81': 128, 'note_on_6_instrument_38': 129, 'note_on_6_instrument_121': 130, 'note_on_7_instrument_80': 131, 'note_on_7_instrument_81': 132, 'note_on_7_instrument_38': 133, 'note_on_7_instrument_121': 134, 'note_on_8_instrument_80': 135, 'note_on_8_instrument_81': 136, 'note_on_8_instrument_38': 137, 'note_on_8_instrument_121': 138, 'note_on_9_instrument_80': 139, 'note_on_9_instrument_81': 140, 'note_on_9_instrument_38': 141, 'note_on_9_instrument_121': 142, 'note_on_10_instrument_80': 143, 'note_on_10_instrument_81': 144, 'note_on_10_instrument_38': 145, 'note_on_10_instrument_121': 146, 'note_on_11_instrument_80': 147, 'note_on_11_instrument_81': 148, 'note_on_11_instrument_38': 149, 'note_on_11_instrument_121': 150, 'note_on_12_instrument_80': 151, 'note_on_12_instrument_81': 152, 'note_on_12_instrument_38': 153, 'note_on_12_instrument_121': 154, 'note_on_13_instrument_80': 155, 'note_on_13_instrument_81': 156, 'note_on_13_instrument_38': 157, 'note_on_13_instrument_121': 158, 'note_on_14_instrument_80': 159, 'note_on_14_instrument_81': 160, 'note_on_14_instrument_38': 161, 'note_on_14_instrument_121': 162, 'note_on_15_instrument_80': 163, 'note_on_15_instrument_81': 164, 'note_on_15_instrument_38': 165, 'note_on_15_instrument_121': 166, 'note_on_16_instrument_80': 167, 'note_on_16_instrument_81': 168, 'note_on_16_instrument_38': 169, 'note_on_16_instrument_121': 170, 'note_on_17_instrument_80': 171, 'note_on_17_instrument_81': 172, 'note_on_17_instrument_38': 173, 'note_on_17_instrument_121': 174, 'note_on_18_instrument_80': 175, 'note_on_18_instrument_81': 176, 'note_on_18_instrument_38': 177, 'note_on_18_instrument_121': 178, 'note_on_19_instrument_80': 179, 'note_on_19_instrument_81': 180, 'note_on_19_instrument_38': 181, 'note_on_19_instrument_121': 182, 'note_on_20_instrument_80': 183, 'note_on_20_instrument_81': 184, 'note_on_20_instrument_38': 185, 'note_on_20_instrument_121': 186, 'note_on_21_instrument_80': 187, 'note_on_21_instrument_81': 188, 'note_on_21_instrument_38': 189, 'note_on_21_instrument_121': 190, 'note_on_22_instrument_80': 191, 'note_on_22_instrument_81': 192, 'note_on_22_instrument_38': 193, 'note_on_22_instrument_121': 194, 'note_on_23_instrument_80': 195, 'note_on_23_instrument_81': 196, 'note_on_23_instrument_38': 197, 'note_on_23_instrument_121': 198, 'note_on_24_instrument_80': 199, 'note_on_24_instrument_81': 200, 'note_on_24_instrument_38': 201, 'note_on_24_instrument_121': 202, 'note_on_25_instrument_80': 203, 'note_on_25_instrument_81': 204, 'note_on_25_instrument_38': 205, 'note_on_25_instrument_121': 206, 'note_on_26_instrument_80': 207, 'note_on_26_instrument_81': 208, 'note_on_26_instrument_38': 209, 'note_on_26_instrument_121': 210, 'note_on_27_instrument_80': 211, 'note_on_27_instrument_81': 212, 'note_on_27_instrument_38': 213, 'note_on_27_instrument_121': 214, 'note_on_28_instrument_80': 215, 'note_on_28_instrument_81': 216, 'note_on_28_instrument_38': 217, 'note_on_28_instrument_121': 218, 'note_on_29_instrument_80': 219, 'note_on_29_instrument_81': 220, 'note_on_29_instrument_38': 221, 'note_on_29_instrument_121': 222, 'note_on_30_instrument_80': 223, 'note_on_30_instrument_81': 224, 'note_on_30_instrument_38': 225, 'note_on_30_instrument_121': 226, 'note_on_31_instrument_80': 227, 'note_on_31_instrument_81': 228, 'note_on_31_instrument_38': 229, 'note_on_31_instrument_121': 230, 'note_on_32_instrument_80': 231, 'note_on_32_instrument_81': 232, 'note_on_32_instrument_38': 233, 'note_on_32_instrument_121': 234, 'note_on_33_instrument_80': 235, 'note_on_33_instrument_81': 236, 'note_on_33_instrument_38': 237, 'note_on_33_instrument_121': 238, 'note_on_34_instrument_80': 239, 'note_on_34_instrument_81': 240, 'note_on_34_instrument_38': 241, 'note_on_34_instrument_121': 242, 'note_on_35_instrument_80': 243, 'note_on_35_instrument_81': 244, 'note_on_35_instrument_38': 245, 'note_on_35_instrument_121': 246, 'note_on_36_instrument_80': 247, 'note_on_36_instrument_81': 248, 'note_on_36_instrument_38': 249, 'note_on_36_instrument_121': 250, 'note_on_37_instrument_80': 251, 'note_on_37_instrument_81': 252, 'note_on_37_instrument_38': 253, 'note_on_37_instrument_121': 254, 'note_on_38_instrument_80': 255, 'note_on_38_instrument_81': 256, 'note_on_38_instrument_38': 257, 'note_on_38_instrument_121': 258, 'note_on_39_instrument_80': 259, 'note_on_39_instrument_81': 260, 'note_on_39_instrument_38': 261, 'note_on_39_instrument_121': 262, 'note_on_40_instrument_80': 263, 'note_on_40_instrument_81': 264, 'note_on_40_instrument_38': 265, 'note_on_40_instrument_121': 266, 'note_on_41_instrument_80': 267, 'note_on_41_instrument_81': 268, 'note_on_41_instrument_38': 269, 'note_on_41_instrument_121': 270, 'note_on_42_instrument_80': 271, 'note_on_42_instrument_81': 272, 'note_on_42_instrument_38': 273, 'note_on_42_instrument_121': 274, 'note_on_43_instrument_80': 275, 'note_on_43_instrument_81': 276, 'note_on_43_instrument_38': 277, 'note_on_43_instrument_121': 278, 'note_on_44_instrument_80': 279, 'note_on_44_instrument_81': 280, 'note_on_44_instrument_38': 281, 'note_on_44_instrument_121': 282, 'note_on_45_instrument_80': 283, 'note_on_45_instrument_81': 284, 'note_on_45_instrument_38': 285, 'note_on_45_instrument_121': 286, 'note_on_46_instrument_80': 287, 'note_on_46_instrument_81': 288, 'note_on_46_instrument_38': 289, 'note_on_46_instrument_121': 290, 'note_on_47_instrument_80': 291, 'note_on_47_instrument_81': 292, 'note_on_47_instrument_38': 293, 'note_on_47_instrument_121': 294, 'note_on_48_instrument_80': 295, 'note_on_48_instrument_81': 296, 'note_on_48_instrument_38': 297, 'note_on_48_instrument_121': 298, 'note_on_49_instrument_80': 299, 'note_on_49_instrument_81': 300, 'note_on_49_instrument_38': 301, 'note_on_49_instrument_121': 302, 'note_on_50_instrument_80': 303, 'note_on_50_instrument_81': 304, 'note_on_50_instrument_38': 305, 'note_on_50_instrument_121': 306, 'note_on_51_instrument_80': 307, 'note_on_51_instrument_81': 308, 'note_on_51_instrument_38': 309, 'note_on_51_instrument_121': 310, 'note_on_52_instrument_80': 311, 'note_on_52_instrument_81': 312, 'note_on_52_instrument_38': 313, 'note_on_52_instrument_121': 314, 'note_on_53_instrument_80': 315, 'note_on_53_instrument_81': 316, 'note_on_53_instrument_38': 317, 'note_on_53_instrument_121': 318, 'note_on_54_instrument_80': 319, 'note_on_54_instrument_81': 320, 'note_on_54_instrument_38': 321, 'note_on_54_instrument_121': 322, 'note_on_55_instrument_80': 323, 'note_on_55_instrument_81': 324, 'note_on_55_instrument_38': 325, 'note_on_55_instrument_121': 326, 'note_on_56_instrument_80': 327, 'note_on_56_instrument_81': 328, 'note_on_56_instrument_38': 329, 'note_on_56_instrument_121': 330, 'note_on_57_instrument_80': 331, 'note_on_57_instrument_81': 332, 'note_on_57_instrument_38': 333, 'note_on_57_instrument_121': 334, 'note_on_58_instrument_80': 335, 'note_on_58_instrument_81': 336, 'note_on_58_instrument_38': 337, 'note_on_58_instrument_121': 338, 'note_on_59_instrument_80': 339, 'note_on_59_instrument_81': 340, 'note_on_59_instrument_38': 341, 'note_on_59_instrument_121': 342, 'note_on_60_instrument_80': 343, 'note_on_60_instrument_81': 344, 'note_on_60_instrument_38': 345, 'note_on_60_instrument_121': 346, 'note_on_61_instrument_80': 347, 'note_on_61_instrument_81': 348, 'note_on_61_instrument_38': 349, 'note_on_61_instrument_121': 350, 'note_on_62_instrument_80': 351, 'note_on_62_instrument_81': 352, 'note_on_62_instrument_38': 353, 'note_on_62_instrument_121': 354, 'note_on_63_instrument_80': 355, 'note_on_63_instrument_81': 356, 'note_on_63_instrument_38': 357, 'note_on_63_instrument_121': 358, 'note_on_64_instrument_80': 359, 'note_on_64_instrument_81': 360, 'note_on_64_instrument_38': 361, 'note_on_64_instrument_121': 362, 'note_on_65_instrument_80': 363, 'note_on_65_instrument_81': 364, 'note_on_65_instrument_38': 365, 'note_on_65_instrument_121': 366, 'note_on_66_instrument_80': 367, 'note_on_66_instrument_81': 368, 'note_on_66_instrument_38': 369, 'note_on_66_instrument_121': 370, 'note_on_67_instrument_80': 371, 'note_on_67_instrument_81': 372, 'note_on_67_instrument_38': 373, 'note_on_67_instrument_121': 374, 'note_on_68_instrument_80': 375, 'note_on_68_instrument_81': 376, 'note_on_68_instrument_38': 377, 'note_on_68_instrument_121': 378, 'note_on_69_instrument_80': 379, 'note_on_69_instrument_81': 380, 'note_on_69_instrument_38': 381, 'note_on_69_instrument_121': 382, 'note_on_70_instrument_80': 383, 'note_on_70_instrument_81': 384, 'note_on_70_instrument_38': 385, 'note_on_70_instrument_121': 386, 'note_on_71_instrument_80': 387, 'note_on_71_instrument_81': 388, 'note_on_71_instrument_38': 389, 'note_on_71_instrument_121': 390, 'note_on_72_instrument_80': 391, 'note_on_72_instrument_81': 392, 'note_on_72_instrument_38': 393, 'note_on_72_instrument_121': 394, 'note_on_73_instrument_80': 395, 'note_on_73_instrument_81': 396, 'note_on_73_instrument_38': 397, 'note_on_73_instrument_121': 398, 'note_on_74_instrument_80': 399, 'note_on_74_instrument_81': 400, 'note_on_74_instrument_38': 401, 'note_on_74_instrument_121': 402, 'note_on_75_instrument_80': 403, 'note_on_75_instrument_81': 404, 'note_on_75_instrument_38': 405, 'note_on_75_instrument_121': 406, 'note_on_76_instrument_80': 407, 'note_on_76_instrument_81': 408, 'note_on_76_instrument_38': 409, 'note_on_76_instrument_121': 410, 'note_on_77_instrument_80': 411, 'note_on_77_instrument_81': 412, 'note_on_77_instrument_38': 413, 'note_on_77_instrument_121': 414, 'note_on_78_instrument_80': 415, 'note_on_78_instrument_81': 416, 'note_on_78_instrument_38': 417, 'note_on_78_instrument_121': 418, 'note_on_79_instrument_80': 419, 'note_on_79_instrument_81': 420, 'note_on_79_instrument_38': 421, 'note_on_79_instrument_121': 422, 'note_on_80_instrument_80': 423, 'note_on_80_instrument_81': 424, 'note_on_80_instrument_38': 425, 'note_on_80_instrument_121': 426, 'note_on_81_instrument_80': 427, 'note_on_81_instrument_81': 428, 'note_on_81_instrument_38': 429, 'note_on_81_instrument_121': 430, 'note_on_82_instrument_80': 431, 'note_on_82_instrument_81': 432, 'note_on_82_instrument_38': 433, 'note_on_82_instrument_121': 434, 'note_on_83_instrument_80': 435, 'note_on_83_instrument_81': 436, 'note_on_83_instrument_38': 437, 'note_on_83_instrument_121': 438, 'note_on_84_instrument_80': 439, 'note_on_84_instrument_81': 440, 'note_on_84_instrument_38': 441, 'note_on_84_instrument_121': 442, 'note_on_85_instrument_80': 443, 'note_on_85_instrument_81': 444, 'note_on_85_instrument_38': 445, 'note_on_85_instrument_121': 446, 'note_on_86_instrument_80': 447, 'note_on_86_instrument_81': 448, 'note_on_86_instrument_38': 449, 'note_on_86_instrument_121': 450, 'note_on_87_instrument_80': 451, 'note_on_87_instrument_81': 452, 'note_on_87_instrument_38': 453, 'note_on_87_instrument_121': 454, 'note_on_88_instrument_80': 455, 'note_on_88_instrument_81': 456, 'note_on_88_instrument_38': 457, 'note_on_88_instrument_121': 458, 'note_on_89_instrument_80': 459, 'note_on_89_instrument_81': 460, 'note_on_89_instrument_38': 461, 'note_on_89_instrument_121': 462, 'note_on_90_instrument_80': 463, 'note_on_90_instrument_81': 464, 'note_on_90_instrument_38': 465, 'note_on_90_instrument_121': 466, 'note_on_91_instrument_80': 467, 'note_on_91_instrument_81': 468, 'note_on_91_instrument_38': 469, 'note_on_91_instrument_121': 470, 'note_on_92_instrument_80': 471, 'note_on_92_instrument_81': 472, 'note_on_92_instrument_38': 473, 'note_on_92_instrument_121': 474, 'note_on_93_instrument_80': 475, 'note_on_93_instrument_81': 476, 'note_on_93_instrument_38': 477, 'note_on_93_instrument_121': 478, 'note_on_94_instrument_80': 479, 'note_on_94_instrument_81': 480, 'note_on_94_instrument_38': 481, 'note_on_94_instrument_121': 482, 'note_on_95_instrument_80': 483, 'note_on_95_instrument_81': 484, 'note_on_95_instrument_38': 485, 'note_on_95_instrument_121': 486, 'note_on_96_instrument_80': 487, 'note_on_96_instrument_81': 488, 'note_on_96_instrument_38': 489, 'note_on_96_instrument_121': 490, 'note_on_97_instrument_80': 491, 'note_on_97_instrument_81': 492, 'note_on_97_instrument_38': 493, 'note_on_97_instrument_121': 494, 'note_on_98_instrument_80': 495, 'note_on_98_instrument_81': 496, 'note_on_98_instrument_38': 497, 'note_on_98_instrument_121': 498, 'note_on_99_instrument_80': 499, 'note_on_99_instrument_81': 500, 'note_on_99_instrument_38': 501, 'note_on_99_instrument_121': 502, 'note_on_100_instrument_80': 503, 'note_on_100_instrument_81': 504, 'note_on_100_instrument_38': 505, 'note_on_100_instrument_121': 506, 'note_on_101_instrument_80': 507, 'note_on_101_instrument_81': 508, 'note_on_101_instrument_38': 509, 'note_on_101_instrument_121': 510, 'note_on_102_instrument_80': 511, 'note_on_102_instrument_81': 512, 'note_on_102_instrument_38': 513, 'note_on_102_instrument_121': 514, 'note_on_103_instrument_80': 515, 'note_on_103_instrument_81': 516, 'note_on_103_instrument_38': 517, 'note_on_103_instrument_121': 518, 'note_on_104_instrument_80': 519, 'note_on_104_instrument_81': 520, 'note_on_104_instrument_38': 521, 'note_on_104_instrument_121': 522, 'note_on_105_instrument_80': 523, 'note_on_105_instrument_81': 524, 'note_on_105_instrument_38': 525, 'note_on_105_instrument_121': 526, 'note_on_106_instrument_80': 527, 'note_on_106_instrument_81': 528, 'note_on_106_instrument_38': 529, 'note_on_106_instrument_121': 530, 'note_on_107_instrument_80': 531, 'note_on_107_instrument_81': 532, 'note_on_107_instrument_38': 533, 'note_on_107_instrument_121': 534, 'note_on_108_instrument_80': 535, 'note_on_108_instrument_81': 536, 'note_on_108_instrument_38': 537, 'note_on_108_instrument_121': 538, 'note_on_109_instrument_80': 539, 'note_on_109_instrument_81': 540, 'note_on_109_instrument_38': 541, 'note_on_109_instrument_121': 542, 'note_on_110_instrument_80': 543, 'note_on_110_instrument_81': 544, 'note_on_110_instrument_38': 545, 'note_on_110_instrument_121': 546, 'note_on_111_instrument_80': 547, 'note_on_111_instrument_81': 548, 'note_on_111_instrument_38': 549, 'note_on_111_instrument_121': 550, 'note_on_112_instrument_80': 551, 'note_on_112_instrument_81': 552, 'note_on_112_instrument_38': 553, 'note_on_112_instrument_121': 554, 'note_on_113_instrument_80': 555, 'note_on_113_instrument_81': 556, 'note_on_113_instrument_38': 557, 'note_on_113_instrument_121': 558, 'note_on_114_instrument_80': 559, 'note_on_114_instrument_81': 560, 'note_on_114_instrument_38': 561, 'note_on_114_instrument_121': 562, 'note_on_115_instrument_80': 563, 'note_on_115_instrument_81': 564, 'note_on_115_instrument_38': 565, 'note_on_115_instrument_121': 566, 'note_on_116_instrument_80': 567, 'note_on_116_instrument_81': 568, 'note_on_116_instrument_38': 569, 'note_on_116_instrument_121': 570, 'note_on_117_instrument_80': 571, 'note_on_117_instrument_81': 572, 'note_on_117_instrument_38': 573, 'note_on_117_instrument_121': 574, 'note_on_118_instrument_80': 575, 'note_on_118_instrument_81': 576, 'note_on_118_instrument_38': 577, 'note_on_118_instrument_121': 578, 'note_on_119_instrument_80': 579, 'note_on_119_instrument_81': 580, 'note_on_119_instrument_38': 581, 'note_on_119_instrument_121': 582, 'note_on_120_instrument_80': 583, 'note_on_120_instrument_81': 584, 'note_on_120_instrument_38': 585, 'note_on_120_instrument_121': 586, 'note_on_121_instrument_80': 587, 'note_on_121_instrument_81': 588, 'note_on_121_instrument_38': 589, 'note_on_121_instrument_121': 590, 'note_on_122_instrument_80': 591, 'note_on_122_instrument_81': 592, 'note_on_122_instrument_38': 593, 'note_on_122_instrument_121': 594, 'note_on_123_instrument_80': 595, 'note_on_123_instrument_81': 596, 'note_on_123_instrument_38': 597, 'note_on_123_instrument_121': 598, 'note_on_124_instrument_80': 599, 'note_on_124_instrument_81': 600, 'note_on_124_instrument_38': 601, 'note_on_124_instrument_121': 602, 'note_on_125_instrument_80': 603, 'note_on_125_instrument_81': 604, 'note_on_125_instrument_38': 605, 'note_on_125_instrument_121': 606, 'note_on_126_instrument_80': 607, 'note_on_126_instrument_81': 608, 'note_on_126_instrument_38': 609, 'note_on_126_instrument_121': 610, 'note_on_127_instrument_80': 611, 'note_on_127_instrument_81': 612, 'note_on_127_instrument_38': 613, 'note_on_127_instrument_121': 614, 'note_off_0_instrument_80': 615, 'note_off_0_instrument_81': 616, 'note_off_0_instrument_38': 617, 'note_off_0_instrument_121': 618, 'note_off_1_instrument_80': 619, 'note_off_1_instrument_81': 620, 'note_off_1_instrument_38': 621, 'note_off_1_instrument_121': 622, 'note_off_2_instrument_80': 623, 'note_off_2_instrument_81': 624, 'note_off_2_instrument_38': 625, 'note_off_2_instrument_121': 626, 'note_off_3_instrument_80': 627, 'note_off_3_instrument_81': 628, 'note_off_3_instrument_38': 629, 'note_off_3_instrument_121': 630, 'note_off_4_instrument_80': 631, 'note_off_4_instrument_81': 632, 'note_off_4_instrument_38': 633, 'note_off_4_instrument_121': 634, 'note_off_5_instrument_80': 635, 'note_off_5_instrument_81': 636, 'note_off_5_instrument_38': 637, 'note_off_5_instrument_121': 638, 'note_off_6_instrument_80': 639, 'note_off_6_instrument_81': 640, 'note_off_6_instrument_38': 641, 'note_off_6_instrument_121': 642, 'note_off_7_instrument_80': 643, 'note_off_7_instrument_81': 644, 'note_off_7_instrument_38': 645, 'note_off_7_instrument_121': 646, 'note_off_8_instrument_80': 647, 'note_off_8_instrument_81': 648, 'note_off_8_instrument_38': 649, 'note_off_8_instrument_121': 650, 'note_off_9_instrument_80': 651, 'note_off_9_instrument_81': 652, 'note_off_9_instrument_38': 653, 'note_off_9_instrument_121': 654, 'note_off_10_instrument_80': 655, 'note_off_10_instrument_81': 656, 'note_off_10_instrument_38': 657, 'note_off_10_instrument_121': 658, 'note_off_11_instrument_80': 659, 'note_off_11_instrument_81': 660, 'note_off_11_instrument_38': 661, 'note_off_11_instrument_121': 662, 'note_off_12_instrument_80': 663, 'note_off_12_instrument_81': 664, 'note_off_12_instrument_38': 665, 'note_off_12_instrument_121': 666, 'note_off_13_instrument_80': 667, 'note_off_13_instrument_81': 668, 'note_off_13_instrument_38': 669, 'note_off_13_instrument_121': 670, 'note_off_14_instrument_80': 671, 'note_off_14_instrument_81': 672, 'note_off_14_instrument_38': 673, 'note_off_14_instrument_121': 674, 'note_off_15_instrument_80': 675, 'note_off_15_instrument_81': 676, 'note_off_15_instrument_38': 677, 'note_off_15_instrument_121': 678, 'note_off_16_instrument_80': 679, 'note_off_16_instrument_81': 680, 'note_off_16_instrument_38': 681, 'note_off_16_instrument_121': 682, 'note_off_17_instrument_80': 683, 'note_off_17_instrument_81': 684, 'note_off_17_instrument_38': 685, 'note_off_17_instrument_121': 686, 'note_off_18_instrument_80': 687, 'note_off_18_instrument_81': 688, 'note_off_18_instrument_38': 689, 'note_off_18_instrument_121': 690, 'note_off_19_instrument_80': 691, 'note_off_19_instrument_81': 692, 'note_off_19_instrument_38': 693, 'note_off_19_instrument_121': 694, 'note_off_20_instrument_80': 695, 'note_off_20_instrument_81': 696, 'note_off_20_instrument_38': 697, 'note_off_20_instrument_121': 698, 'note_off_21_instrument_80': 699, 'note_off_21_instrument_81': 700, 'note_off_21_instrument_38': 701, 'note_off_21_instrument_121': 702, 'note_off_22_instrument_80': 703, 'note_off_22_instrument_81': 704, 'note_off_22_instrument_38': 705, 'note_off_22_instrument_121': 706, 'note_off_23_instrument_80': 707, 'note_off_23_instrument_81': 708, 'note_off_23_instrument_38': 709, 'note_off_23_instrument_121': 710, 'note_off_24_instrument_80': 711, 'note_off_24_instrument_81': 712, 'note_off_24_instrument_38': 713, 'note_off_24_instrument_121': 714, 'note_off_25_instrument_80': 715, 'note_off_25_instrument_81': 716, 'note_off_25_instrument_38': 717, 'note_off_25_instrument_121': 718, 'note_off_26_instrument_80': 719, 'note_off_26_instrument_81': 720, 'note_off_26_instrument_38': 721, 'note_off_26_instrument_121': 722, 'note_off_27_instrument_80': 723, 'note_off_27_instrument_81': 724, 'note_off_27_instrument_38': 725, 'note_off_27_instrument_121': 726, 'note_off_28_instrument_80': 727, 'note_off_28_instrument_81': 728, 'note_off_28_instrument_38': 729, 'note_off_28_instrument_121': 730, 'note_off_29_instrument_80': 731, 'note_off_29_instrument_81': 732, 'note_off_29_instrument_38': 733, 'note_off_29_instrument_121': 734, 'note_off_30_instrument_80': 735, 'note_off_30_instrument_81': 736, 'note_off_30_instrument_38': 737, 'note_off_30_instrument_121': 738, 'note_off_31_instrument_80': 739, 'note_off_31_instrument_81': 740, 'note_off_31_instrument_38': 741, 'note_off_31_instrument_121': 742, 'note_off_32_instrument_80': 743, 'note_off_32_instrument_81': 744, 'note_off_32_instrument_38': 745, 'note_off_32_instrument_121': 746, 'note_off_33_instrument_80': 747, 'note_off_33_instrument_81': 748, 'note_off_33_instrument_38': 749, 'note_off_33_instrument_121': 750, 'note_off_34_instrument_80': 751, 'note_off_34_instrument_81': 752, 'note_off_34_instrument_38': 753, 'note_off_34_instrument_121': 754, 'note_off_35_instrument_80': 755, 'note_off_35_instrument_81': 756, 'note_off_35_instrument_38': 757, 'note_off_35_instrument_121': 758, 'note_off_36_instrument_80': 759, 'note_off_36_instrument_81': 760, 'note_off_36_instrument_38': 761, 'note_off_36_instrument_121': 762, 'note_off_37_instrument_80': 763, 'note_off_37_instrument_81': 764, 'note_off_37_instrument_38': 765, 'note_off_37_instrument_121': 766, 'note_off_38_instrument_80': 767, 'note_off_38_instrument_81': 768, 'note_off_38_instrument_38': 769, 'note_off_38_instrument_121': 770, 'note_off_39_instrument_80': 771, 'note_off_39_instrument_81': 772, 'note_off_39_instrument_38': 773, 'note_off_39_instrument_121': 774, 'note_off_40_instrument_80': 775, 'note_off_40_instrument_81': 776, 'note_off_40_instrument_38': 777, 'note_off_40_instrument_121': 778, 'note_off_41_instrument_80': 779, 'note_off_41_instrument_81': 780, 'note_off_41_instrument_38': 781, 'note_off_41_instrument_121': 782, 'note_off_42_instrument_80': 783, 'note_off_42_instrument_81': 784, 'note_off_42_instrument_38': 785, 'note_off_42_instrument_121': 786, 'note_off_43_instrument_80': 787, 'note_off_43_instrument_81': 788, 'note_off_43_instrument_38': 789, 'note_off_43_instrument_121': 790, 'note_off_44_instrument_80': 791, 'note_off_44_instrument_81': 792, 'note_off_44_instrument_38': 793, 'note_off_44_instrument_121': 794, 'note_off_45_instrument_80': 795, 'note_off_45_instrument_81': 796, 'note_off_45_instrument_38': 797, 'note_off_45_instrument_121': 798, 'note_off_46_instrument_80': 799, 'note_off_46_instrument_81': 800, 'note_off_46_instrument_38': 801, 'note_off_46_instrument_121': 802, 'note_off_47_instrument_80': 803, 'note_off_47_instrument_81': 804, 'note_off_47_instrument_38': 805, 'note_off_47_instrument_121': 806, 'note_off_48_instrument_80': 807, 'note_off_48_instrument_81': 808, 'note_off_48_instrument_38': 809, 'note_off_48_instrument_121': 810, 'note_off_49_instrument_80': 811, 'note_off_49_instrument_81': 812, 'note_off_49_instrument_38': 813, 'note_off_49_instrument_121': 814, 'note_off_50_instrument_80': 815, 'note_off_50_instrument_81': 816, 'note_off_50_instrument_38': 817, 'note_off_50_instrument_121': 818, 'note_off_51_instrument_80': 819, 'note_off_51_instrument_81': 820, 'note_off_51_instrument_38': 821, 'note_off_51_instrument_121': 822, 'note_off_52_instrument_80': 823, 'note_off_52_instrument_81': 824, 'note_off_52_instrument_38': 825, 'note_off_52_instrument_121': 826, 'note_off_53_instrument_80': 827, 'note_off_53_instrument_81': 828, 'note_off_53_instrument_38': 829, 'note_off_53_instrument_121': 830, 'note_off_54_instrument_80': 831, 'note_off_54_instrument_81': 832, 'note_off_54_instrument_38': 833, 'note_off_54_instrument_121': 834, 'note_off_55_instrument_80': 835, 'note_off_55_instrument_81': 836, 'note_off_55_instrument_38': 837, 'note_off_55_instrument_121': 838, 'note_off_56_instrument_80': 839, 'note_off_56_instrument_81': 840, 'note_off_56_instrument_38': 841, 'note_off_56_instrument_121': 842, 'note_off_57_instrument_80': 843, 'note_off_57_instrument_81': 844, 'note_off_57_instrument_38': 845, 'note_off_57_instrument_121': 846, 'note_off_58_instrument_80': 847, 'note_off_58_instrument_81': 848, 'note_off_58_instrument_38': 849, 'note_off_58_instrument_121': 850, 'note_off_59_instrument_80': 851, 'note_off_59_instrument_81': 852, 'note_off_59_instrument_38': 853, 'note_off_59_instrument_121': 854, 'note_off_60_instrument_80': 855, 'note_off_60_instrument_81': 856, 'note_off_60_instrument_38': 857, 'note_off_60_instrument_121': 858, 'note_off_61_instrument_80': 859, 'note_off_61_instrument_81': 860, 'note_off_61_instrument_38': 861, 'note_off_61_instrument_121': 862, 'note_off_62_instrument_80': 863, 'note_off_62_instrument_81': 864, 'note_off_62_instrument_38': 865, 'note_off_62_instrument_121': 866, 'note_off_63_instrument_80': 867, 'note_off_63_instrument_81': 868, 'note_off_63_instrument_38': 869, 'note_off_63_instrument_121': 870, 'note_off_64_instrument_80': 871, 'note_off_64_instrument_81': 872, 'note_off_64_instrument_38': 873, 'note_off_64_instrument_121': 874, 'note_off_65_instrument_80': 875, 'note_off_65_instrument_81': 876, 'note_off_65_instrument_38': 877, 'note_off_65_instrument_121': 878, 'note_off_66_instrument_80': 879, 'note_off_66_instrument_81': 880, 'note_off_66_instrument_38': 881, 'note_off_66_instrument_121': 882, 'note_off_67_instrument_80': 883, 'note_off_67_instrument_81': 884, 'note_off_67_instrument_38': 885, 'note_off_67_instrument_121': 886, 'note_off_68_instrument_80': 887, 'note_off_68_instrument_81': 888, 'note_off_68_instrument_38': 889, 'note_off_68_instrument_121': 890, 'note_off_69_instrument_80': 891, 'note_off_69_instrument_81': 892, 'note_off_69_instrument_38': 893, 'note_off_69_instrument_121': 894, 'note_off_70_instrument_80': 895, 'note_off_70_instrument_81': 896, 'note_off_70_instrument_38': 897, 'note_off_70_instrument_121': 898, 'note_off_71_instrument_80': 899, 'note_off_71_instrument_81': 900, 'note_off_71_instrument_38': 901, 'note_off_71_instrument_121': 902, 'note_off_72_instrument_80': 903, 'note_off_72_instrument_81': 904, 'note_off_72_instrument_38': 905, 'note_off_72_instrument_121': 906, 'note_off_73_instrument_80': 907, 'note_off_73_instrument_81': 908, 'note_off_73_instrument_38': 909, 'note_off_73_instrument_121': 910, 'note_off_74_instrument_80': 911, 'note_off_74_instrument_81': 912, 'note_off_74_instrument_38': 913, 'note_off_74_instrument_121': 914, 'note_off_75_instrument_80': 915, 'note_off_75_instrument_81': 916, 'note_off_75_instrument_38': 917, 'note_off_75_instrument_121': 918, 'note_off_76_instrument_80': 919, 'note_off_76_instrument_81': 920, 'note_off_76_instrument_38': 921, 'note_off_76_instrument_121': 922, 'note_off_77_instrument_80': 923, 'note_off_77_instrument_81': 924, 'note_off_77_instrument_38': 925, 'note_off_77_instrument_121': 926, 'note_off_78_instrument_80': 927, 'note_off_78_instrument_81': 928, 'note_off_78_instrument_38': 929, 'note_off_78_instrument_121': 930, 'note_off_79_instrument_80': 931, 'note_off_79_instrument_81': 932, 'note_off_79_instrument_38': 933, 'note_off_79_instrument_121': 934, 'note_off_80_instrument_80': 935, 'note_off_80_instrument_81': 936, 'note_off_80_instrument_38': 937, 'note_off_80_instrument_121': 938, 'note_off_81_instrument_80': 939, 'note_off_81_instrument_81': 940, 'note_off_81_instrument_38': 941, 'note_off_81_instrument_121': 942, 'note_off_82_instrument_80': 943, 'note_off_82_instrument_81': 944, 'note_off_82_instrument_38': 945, 'note_off_82_instrument_121': 946, 'note_off_83_instrument_80': 947, 'note_off_83_instrument_81': 948, 'note_off_83_instrument_38': 949, 'note_off_83_instrument_121': 950, 'note_off_84_instrument_80': 951, 'note_off_84_instrument_81': 952, 'note_off_84_instrument_38': 953, 'note_off_84_instrument_121': 954, 'note_off_85_instrument_80': 955, 'note_off_85_instrument_81': 956, 'note_off_85_instrument_38': 957, 'note_off_85_instrument_121': 958, 'note_off_86_instrument_80': 959, 'note_off_86_instrument_81': 960, 'note_off_86_instrument_38': 961, 'note_off_86_instrument_121': 962, 'note_off_87_instrument_80': 963, 'note_off_87_instrument_81': 964, 'note_off_87_instrument_38': 965, 'note_off_87_instrument_121': 966, 'note_off_88_instrument_80': 967, 'note_off_88_instrument_81': 968, 'note_off_88_instrument_38': 969, 'note_off_88_instrument_121': 970, 'note_off_89_instrument_80': 971, 'note_off_89_instrument_81': 972, 'note_off_89_instrument_38': 973, 'note_off_89_instrument_121': 974, 'note_off_90_instrument_80': 975, 'note_off_90_instrument_81': 976, 'note_off_90_instrument_38': 977, 'note_off_90_instrument_121': 978, 'note_off_91_instrument_80': 979, 'note_off_91_instrument_81': 980, 'note_off_91_instrument_38': 981, 'note_off_91_instrument_121': 982, 'note_off_92_instrument_80': 983, 'note_off_92_instrument_81': 984, 'note_off_92_instrument_38': 985, 'note_off_92_instrument_121': 986, 'note_off_93_instrument_80': 987, 'note_off_93_instrument_81': 988, 'note_off_93_instrument_38': 989, 'note_off_93_instrument_121': 990, 'note_off_94_instrument_80': 991, 'note_off_94_instrument_81': 992, 'note_off_94_instrument_38': 993, 'note_off_94_instrument_121': 994, 'note_off_95_instrument_80': 995, 'note_off_95_instrument_81': 996, 'note_off_95_instrument_38': 997, 'note_off_95_instrument_121': 998, 'note_off_96_instrument_80': 999, 'note_off_96_instrument_81': 1000, 'note_off_96_instrument_38': 1001, 'note_off_96_instrument_121': 1002, 'note_off_97_instrument_80': 1003, 'note_off_97_instrument_81': 1004, 'note_off_97_instrument_38': 1005, 'note_off_97_instrument_121': 1006, 'note_off_98_instrument_80': 1007, 'note_off_98_instrument_81': 1008, 'note_off_98_instrument_38': 1009, 'note_off_98_instrument_121': 1010, 'note_off_99_instrument_80': 1011, 'note_off_99_instrument_81': 1012, 'note_off_99_instrument_38': 1013, 'note_off_99_instrument_121': 1014, 'note_off_100_instrument_80': 1015, 'note_off_100_instrument_81': 1016, 'note_off_100_instrument_38': 1017, 'note_off_100_instrument_121': 1018, 'note_off_101_instrument_80': 1019, 'note_off_101_instrument_81': 1020, 'note_off_101_instrument_38': 1021, 'note_off_101_instrument_121': 1022, 'note_off_102_instrument_80': 1023, 'note_off_102_instrument_81': 1024, 'note_off_102_instrument_38': 1025, 'note_off_102_instrument_121': 1026, 'note_off_103_instrument_80': 1027, 'note_off_103_instrument_81': 1028, 'note_off_103_instrument_38': 1029, 'note_off_103_instrument_121': 1030, 'note_off_104_instrument_80': 1031, 'note_off_104_instrument_81': 1032, 'note_off_104_instrument_38': 1033, 'note_off_104_instrument_121': 1034, 'note_off_105_instrument_80': 1035, 'note_off_105_instrument_81': 1036, 'note_off_105_instrument_38': 1037, 'note_off_105_instrument_121': 1038, 'note_off_106_instrument_80': 1039, 'note_off_106_instrument_81': 1040, 'note_off_106_instrument_38': 1041, 'note_off_106_instrument_121': 1042, 'note_off_107_instrument_80': 1043, 'note_off_107_instrument_81': 1044, 'note_off_107_instrument_38': 1045, 'note_off_107_instrument_121': 1046, 'note_off_108_instrument_80': 1047, 'note_off_108_instrument_81': 1048, 'note_off_108_instrument_38': 1049, 'note_off_108_instrument_121': 1050, 'note_off_109_instrument_80': 1051, 'note_off_109_instrument_81': 1052, 'note_off_109_instrument_38': 1053, 'note_off_109_instrument_121': 1054, 'note_off_110_instrument_80': 1055, 'note_off_110_instrument_81': 1056, 'note_off_110_instrument_38': 1057, 'note_off_110_instrument_121': 1058, 'note_off_111_instrument_80': 1059, 'note_off_111_instrument_81': 1060, 'note_off_111_instrument_38': 1061, 'note_off_111_instrument_121': 1062, 'note_off_112_instrument_80': 1063, 'note_off_112_instrument_81': 1064, 'note_off_112_instrument_38': 1065, 'note_off_112_instrument_121': 1066, 'note_off_113_instrument_80': 1067, 'note_off_113_instrument_81': 1068, 'note_off_113_instrument_38': 1069, 'note_off_113_instrument_121': 1070, 'note_off_114_instrument_80': 1071, 'note_off_114_instrument_81': 1072, 'note_off_114_instrument_38': 1073, 'note_off_114_instrument_121': 1074, 'note_off_115_instrument_80': 1075, 'note_off_115_instrument_81': 1076, 'note_off_115_instrument_38': 1077, 'note_off_115_instrument_121': 1078, 'note_off_116_instrument_80': 1079, 'note_off_116_instrument_81': 1080, 'note_off_116_instrument_38': 1081, 'note_off_116_instrument_121': 1082, 'note_off_117_instrument_80': 1083, 'note_off_117_instrument_81': 1084, 'note_off_117_instrument_38': 1085, 'note_off_117_instrument_121': 1086, 'note_off_118_instrument_80': 1087, 'note_off_118_instrument_81': 1088, 'note_off_118_instrument_38': 1089, 'note_off_118_instrument_121': 1090, 'note_off_119_instrument_80': 1091, 'note_off_119_instrument_81': 1092, 'note_off_119_instrument_38': 1093, 'note_off_119_instrument_121': 1094, 'note_off_120_instrument_80': 1095, 'note_off_120_instrument_81': 1096, 'note_off_120_instrument_38': 1097, 'note_off_120_instrument_121': 1098, 'note_off_121_instrument_80': 1099, 'note_off_121_instrument_81': 1100, 'note_off_121_instrument_38': 1101, 'note_off_121_instrument_121': 1102, 'note_off_122_instrument_80': 1103, 'note_off_122_instrument_81': 1104, 'note_off_122_instrument_38': 1105, 'note_off_122_instrument_121': 1106, 'note_off_123_instrument_80': 1107, 'note_off_123_instrument_81': 1108, 'note_off_123_instrument_38': 1109, 'note_off_123_instrument_121': 1110, 'note_off_124_instrument_80': 1111, 'note_off_124_instrument_81': 1112, 'note_off_124_instrument_38': 1113, 'note_off_124_instrument_121': 1114, 'note_off_125_instrument_80': 1115, 'note_off_125_instrument_81': 1116, 'note_off_125_instrument_38': 1117, 'note_off_125_instrument_121': 1118, 'note_off_126_instrument_80': 1119, 'note_off_126_instrument_81': 1120, 'note_off_126_instrument_38': 1121, 'note_off_126_instrument_121': 1122, 'note_off_127_instrument_80': 1123, 'note_off_127_instrument_81': 1124, 'note_off_127_instrument_38': 1125, 'note_off_127_instrument_121': 1126}\n"
     ]
    }
   ],
   "source": [
    "TIME_SHIFT_RESOLUTION = 0.01  # 50 ms\n",
    "MAX_SHIFT_STEPS = 100  # Max 5 seconds\n",
    "BEGINNING_OF_SONG_TOKEN = '<BOS>'\n",
    "END_OF_SONG_TOKEN = '<EOS>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "VOCABULARY = dict()\n",
    "index = 0\n",
    "\n",
    "for special_token in [PAD_TOKEN, BEGINNING_OF_SONG_TOKEN, END_OF_SONG_TOKEN]:\n",
    "    VOCABULARY[special_token] = index\n",
    "    index += 1\n",
    "\n",
    "for time_shift in range(1, MAX_SHIFT_STEPS + 1):\n",
    "    VOCABULARY[f'time_shift_{time_shift}'] = index\n",
    "    index += 1\n",
    "\n",
    "for action in [\"note_on\", \"note_off\"]:\n",
    "    for pitch in range(128):\n",
    "        for program in [80, 81, 38, 121]:\n",
    "            VOCABULARY[f'{action}_{pitch}_instrument_{program}'] = index\n",
    "            index += 1\n",
    "\n",
    "print(f'Vocabulary size: {len(VOCABULARY)}')\n",
    "print(f'Vocabulary: {VOCABULARY}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217eeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_TO_TOKEN = {v: k for k, v in VOCABULARY.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752166e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_tokens(pm: pretty_midi.PrettyMIDI):\n",
    "    events = []\n",
    "\n",
    "    for instrument in pm.instruments:\n",
    "        for note in instrument.notes:\n",
    "            events.append((note.start, f'note_on_{note.pitch}_instrument_{instrument.program}'))\n",
    "            events.append((note.end, f'note_off_{note.pitch}_instrument_{instrument.program}'))\n",
    "    \n",
    "    events.sort()  # Sort by time\n",
    "\n",
    "    tokens = []\n",
    "    last_time = 0.0\n",
    "    for time, event in events:\n",
    "        delta = time - last_time\n",
    "        steps = round(delta / TIME_SHIFT_RESOLUTION)\n",
    "\n",
    "        while steps > 0:\n",
    "            shift = min(steps, MAX_SHIFT_STEPS)\n",
    "            tokens.append(f'time_shift_{shift}')\n",
    "            steps -= shift\n",
    "        \n",
    "        tokens.append(event)\n",
    "        last_time = time\n",
    "    return [BEGINNING_OF_SONG_TOKEN] + tokens + [END_OF_SONG_TOKEN]\n",
    "\n",
    "def tokens_to_midi(tokens):\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instruments = dict()\n",
    "    active_notes = dict()\n",
    "\n",
    "    current_time = 0.0\n",
    "    for token in tokens:\n",
    "        if token.startswith('time_shift_'):\n",
    "            shift_steps = int(token.split('_')[-1])\n",
    "            current_time += shift_steps * TIME_SHIFT_RESOLUTION\n",
    "        elif token.startswith('note_on_'):\n",
    "            pitch = int(token.split('_')[2])\n",
    "            instrument = int(token.split('_')[-1])\n",
    "            active_notes[(pitch, instrument)] = current_time\n",
    "        elif token.startswith('note_off_'):\n",
    "            pitch = int(token.split('_')[2])\n",
    "            instrument = int(token.split('_')[-1])\n",
    "            if (pitch, instrument) not in active_notes:\n",
    "                print(f\"Warning: Note off for {pitch} on instrument {instrument} without matching note on.\")\n",
    "                continue\n",
    "            start_time = active_notes[(pitch, instrument)]\n",
    "\n",
    "            if instrument not in instruments:\n",
    "                instruments[instrument] = pretty_midi.Instrument(program=instrument)\n",
    "\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100, pitch=pitch, start=start_time, end=current_time\n",
    "            )\n",
    "\n",
    "            instruments[instrument].notes.append(note)\n",
    "            \n",
    "    for instrument in instruments.values():\n",
    "        pm.instruments.append(instrument)\n",
    "    \n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6adb1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'time_shift_1', 'note_on_62_instrument_38', 'note_on_65_instrument_80', 'time_shift_39', 'note_off_65_instrument_80', 'note_on_69_instrument_80', 'note_off_62_instrument_38', 'note_on_65_instrument_38', 'time_shift_13', 'note_off_69_instrument_80', 'note_on_65_instrument_80', 'note_off_65_instrument_38', 'note_on_62_instrument_38', 'time_shift_13', 'note_off_65_instrument_80', 'note_on_69_instrument_80', 'note_off_62_instrument_38', 'note_on_65_instrument_38', 'time_shift_13', 'note_off_69_instrument_80', 'note_on_70_instrument_80', 'note_off_65_instrument_38', 'note_on_67_instrument_38', 'time_shift_40', 'note_off_70_instrument_80', 'note_on_74_instrument_80', 'note_off_67_instrument_38', 'note_on_70_instrument_38', 'time_shift_13']\n"
     ]
    }
   ],
   "source": [
    "midi = pretty_midi.PrettyMIDI(all_filepaths[0])\n",
    "tokens = midi_to_tokens(midi)\n",
    "print(tokens[:30])\n",
    "midi_reconstructed = tokens_to_midi(tokens)\n",
    "# midi_reconstructed.write('reconstructed_midi.mid')\n",
    "# midi.write('original_midi.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc857af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences(filepaths):\n",
    "    sequences = []\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        pm = pretty_midi.PrettyMIDI(filepath)\n",
    "        tokens = midi_to_tokens(pm)\n",
    "        if not tokens:\n",
    "            raise ValueError(f'No tokens generated for {filepath}')\n",
    "        sequences.append([VOCABULARY[token] for token in tokens])\n",
    "        print_progress_bar(i+1, len(filepaths), prefix='Loading sequences')\n",
    "    return sequences\n",
    "\n",
    "class MIDITokenDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_length=512):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        for ind, seq in enumerate(sequences):\n",
    "            num_chunks = len(seq) // (seq_length + 1)\n",
    "            for chunk in range(num_chunks + 1):\n",
    "                chunk_start = chunk * (seq_length + 1)\n",
    "                chunk_end = chunk_start + seq_length + 1\n",
    "                chunk = seq[chunk_start:chunk_end]\n",
    "\n",
    "                if len(chunk) < seq_length / 4:\n",
    "                    continue\n",
    "\n",
    "                input = chunk[:-1]\n",
    "                input = np.pad(input, (0, seq_length - len(input)), constant_values=VOCABULARY[PAD_TOKEN])\n",
    "                \n",
    "                target = chunk[1:]\n",
    "                target = np.pad(target, (0, seq_length - len(target)), constant_values=VOCABULARY[PAD_TOKEN])\n",
    "                \n",
    "                self.inputs.append(torch.tensor(input, dtype=torch.long))\n",
    "                self.targets.append(torch.tensor(target, dtype=torch.long))\n",
    "            \n",
    "            print_progress_bar(ind+1, len(sequences), prefix='Processing sequences')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b488f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Loading sequences |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Load and convert\n",
    "train_sequences = load_sequences(midi_train_filepaths)\n",
    "val_sequences = load_sequences(midi_val_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b0192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: 4470\n",
      "Train 90th percile length: 4361.699999999999\n",
      "Train 50th percile length: 944.0\n",
      "Train 25th percile length: 286.25\n",
      "Train 10th percile length: 99.0\n",
      "Train max train sequence length: 27506\n",
      "Train min train sequence length: 7\n",
      "Validation sequences: 402\n",
      "Validation 90th percile length: 4329.300000000001\n",
      "Validation 50th percile length: 682.0\n",
      "Validation 25th percile length: 176.0\n",
      "Validation 10th percile length: 73.0\n",
      "Validation max train sequence length: 14466\n",
      "Validation min train sequence length: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_sequence_percentiles(sequences, prefix=''):\n",
    "    print(f'{prefix} sequences: {len(sequences)}')\n",
    "    print(f\"{prefix} 90th percile length: {np.percentile([len(seq) for seq in sequences], 90)}\")\n",
    "    print(f\"{prefix} 50th percile length: {np.percentile([len(seq) for seq in sequences], 50)}\")\n",
    "    print(f\"{prefix} 25th percile length: {np.percentile([len(seq) for seq in sequences], 25)}\")\n",
    "    print(f\"{prefix} 10th percile length: {np.percentile([len(seq) for seq in sequences], 10)}\")\n",
    "    print(f\"{prefix} max train sequence length: {max(len(seq) for seq in sequences)}\")\n",
    "    print(f\"{prefix} min train sequence length: {min(len(seq) for seq in sequences)}\")\n",
    "\n",
    "print_sequence_percentiles(train_sequences, prefix='Train')\n",
    "print_sequence_percentiles(val_sequences, prefix='Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf1a698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Train dataset size: 16287\n",
      "Validation dataset size: 1328\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = MIDITokenDataset(train_sequences, seq_length=512)\n",
    "val_dataset = MIDITokenDataset(val_sequences, seq_length=512)\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355a8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch_size, seq_length)\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        out, hidden = self.rnn(x, hidden)  # out: (batch_size, seq_length, hidden_dim)\n",
    "        out = self.fc(out)  # (batch_size, seq_length, vocab_size)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec47d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device='cuda'):\n",
    "    time_start = time.time()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --------- Training ---------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            print_progress_bar(i+1, len(train_loader), prefix='Training...')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --------- Validation ---------\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs, _ = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                print_progress_bar(i+1, len(val_loader), prefix='Validating...')\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "058266e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 1/20 | Train Loss: 6.9929 | Val Loss: 6.8670\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 2/20 | Train Loss: 6.8542 | Val Loss: 6.6067\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 3/20 | Train Loss: 6.5978 | Val Loss: 6.2484\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 4/20 | Train Loss: 6.1302 | Val Loss: 5.6940\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 5/20 | Train Loss: 5.4105 | Val Loss: 5.1949\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 6/20 | Train Loss: 4.8852 | Val Loss: 5.1448\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 7/20 | Train Loss: 4.6840 | Val Loss: 5.1625\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 8/20 | Train Loss: 4.5786 | Val Loss: 5.1867\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 9/20 | Train Loss: 4.5283 | Val Loss: 5.2022\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 10/20 | Train Loss: 4.5020 | Val Loss: 5.1200\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 11/20 | Train Loss: 4.4670 | Val Loss: 5.0982\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 12/20 | Train Loss: 4.4280 | Val Loss: 5.0323\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 13/20 | Train Loss: 4.3875 | Val Loss: 4.9974\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 14/20 | Train Loss: 4.3469 | Val Loss: 4.9794\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 15/20 | Train Loss: 4.3182 | Val Loss: 4.9795\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 16/20 | Train Loss: 4.2979 | Val Loss: 4.9992\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 17/20 | Train Loss: 4.2652 | Val Loss: 5.0171\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 18/20 | Train Loss: 4.2405 | Val Loss: 5.0300\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 19/20 | Train Loss: 4.2147 | Val Loss: 5.0580\n",
      "Training... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating... |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Epoch 20/20 | Train Loss: 4.2022 | Val Loss: 5.0812\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # You can adjust this depending on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MusicRNN(vocab_size=len(VOCABULARY), embedding_dim=256, hidden_dim=256, num_layers=2)\n",
    "train(model, train_loader, val_loader, vocab_size=len(VOCABULARY), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4135d2-b8b5-439b-b131-35bbc4e44efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd31755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, start_token, max_length=100, temperature=1.0, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_token = torch.tensor([[start_token]], device=device)  # (1, 1)\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input_token, hidden)  # output: (1, 1, vocab_size)\n",
    "        output = output[:, -1, :]  # take the last output\n",
    "        output = output / temperature  # adjust randomness\n",
    "\n",
    "        probs = F.softmax(output, dim=-1)  # (1, vocab_size)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == VOCABULARY[END_OF_SONG_TOKEN] or VOCABULARY[PAD_TOKEN]: # reach end of sequence\n",
    "          break\n",
    "\n",
    "        input_token = torch.tensor([[next_token]], device=device)\n",
    "\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6396f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Note off for 71 on instrument 80 without matching note on.\n",
      "Warning: Note off for 57 on instrument 81 without matching note on.\n",
      "Warning: Note off for 58 on instrument 80 without matching note on.\n",
      "Warning: Note off for 4 on instrument 38 without matching note on.\n",
      "Warning: Note off for 55 on instrument 38 without matching note on.\n",
      "Warning: Note off for 13 on instrument 121 without matching note on.\n",
      "Warning: Note off for 69 on instrument 80 without matching note on.\n",
      "Warning: Note off for 47 on instrument 38 without matching note on.\n",
      "Warning: Note off for 57 on instrument 81 without matching note on.\n",
      "Warning: Note off for 36 on instrument 81 without matching note on.\n",
      "Warning: Note off for 40 on instrument 38 without matching note on.\n",
      "Warning: Note off for 82 on instrument 80 without matching note on.\n",
      "Warning: Note off for 56 on instrument 80 without matching note on.\n",
      "Warning: Note off for 35 on instrument 80 without matching note on.\n",
      "Warning: Note off for 69 on instrument 80 without matching note on.\n",
      "Warning: Note off for 64 on instrument 81 without matching note on.\n",
      "Warning: Note off for 76 on instrument 80 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 58 on instrument 80 without matching note on.\n",
      "Warning: Note off for 55 on instrument 81 without matching note on.\n",
      "Warning: Note off for 37 on instrument 38 without matching note on.\n",
      "Warning: Note off for 36 on instrument 81 without matching note on.\n",
      "Warning: Note off for 59 on instrument 81 without matching note on.\n",
      "Warning: Note off for 84 on instrument 80 without matching note on.\n",
      "Warning: Note off for 52 on instrument 80 without matching note on.\n",
      "Warning: Note off for 53 on instrument 80 without matching note on.\n",
      "Warning: Note off for 43 on instrument 81 without matching note on.\n",
      "Warning: Note off for 41 on instrument 81 without matching note on.\n",
      "Warning: Note off for 40 on instrument 81 without matching note on.\n",
      "Warning: Note off for 69 on instrument 81 without matching note on.\n",
      "Warning: Note off for 83 on instrument 81 without matching note on.\n",
      "Warning: Note off for 62 on instrument 80 without matching note on.\n",
      "Warning: Note off for 72 on instrument 81 without matching note on.\n",
      "Warning: Note off for 58 on instrument 81 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 83 on instrument 80 without matching note on.\n",
      "Warning: Note off for 42 on instrument 38 without matching note on.\n",
      "Warning: Note off for 40 on instrument 38 without matching note on.\n",
      "Warning: Note off for 52 on instrument 38 without matching note on.\n",
      "Warning: Note off for 35 on instrument 38 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 43 on instrument 81 without matching note on.\n",
      "Warning: Note off for 33 on instrument 80 without matching note on.\n",
      "Warning: Note off for 42 on instrument 38 without matching note on.\n",
      "Warning: Note off for 42 on instrument 38 without matching note on.\n",
      "Warning: Note off for 81 on instrument 80 without matching note on.\n",
      "Warning: Note off for 74 on instrument 81 without matching note on.\n",
      "Warning: Note off for 65 on instrument 81 without matching note on.\n",
      "Warning: Note off for 76 on instrument 80 without matching note on.\n",
      "Warning: Note off for 82 on instrument 80 without matching note on.\n",
      "Warning: Note off for 80 on instrument 80 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 34 on instrument 81 without matching note on.\n",
      "Warning: Note off for 36 on instrument 38 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 73 on instrument 80 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 68 on instrument 80 without matching note on.\n",
      "Warning: Note off for 69 on instrument 38 without matching note on.\n",
      "Warning: Note off for 67 on instrument 80 without matching note on.\n",
      "Warning: Note off for 41 on instrument 81 without matching note on.\n",
      "Warning: Note off for 71 on instrument 81 without matching note on.\n",
      "Warning: Note off for 54 on instrument 38 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 63 on instrument 81 without matching note on.\n",
      "Warning: Note off for 75 on instrument 81 without matching note on.\n",
      "Warning: Note off for 71 on instrument 38 without matching note on.\n",
      "Warning: Note off for 35 on instrument 38 without matching note on.\n",
      "Warning: Note off for 41 on instrument 80 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 74 on instrument 38 without matching note on.\n",
      "Warning: Note off for 88 on instrument 80 without matching note on.\n",
      "Warning: Note off for 69 on instrument 81 without matching note on.\n",
      "Warning: Note off for 72 on instrument 80 without matching note on.\n",
      "Warning: Note off for 79 on instrument 80 without matching note on.\n",
      "Warning: Note off for 63 on instrument 81 without matching note on.\n",
      "Warning: Note off for 36 on instrument 38 without matching note on.\n",
      "Warning: Note off for 84 on instrument 80 without matching note on.\n",
      "Warning: Note off for 63 on instrument 81 without matching note on.\n",
      "Warning: Note off for 54 on instrument 38 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 38 on instrument 38 without matching note on.\n",
      "Warning: Note off for 81 on instrument 81 without matching note on.\n",
      "Warning: Note off for 38 on instrument 38 without matching note on.\n",
      "Warning: Note off for 84 on instrument 80 without matching note on.\n",
      "Warning: Note off for 57 on instrument 38 without matching note on.\n",
      "Warning: Note off for 94 on instrument 81 without matching note on.\n",
      "Warning: Note off for 36 on instrument 38 without matching note on.\n",
      "Warning: Note off for 55 on instrument 80 without matching note on.\n",
      "Warning: Note off for 77 on instrument 38 without matching note on.\n",
      "Warning: Note off for 54 on instrument 38 without matching note on.\n",
      "Warning: Note off for 70 on instrument 81 without matching note on.\n",
      "Warning: Note off for 67 on instrument 80 without matching note on.\n",
      "Warning: Note off for 87 on instrument 81 without matching note on.\n",
      "Warning: Note off for 86 on instrument 38 without matching note on.\n",
      "Warning: Note off for 86 on instrument 38 without matching note on.\n",
      "Warning: Note off for 79 on instrument 80 without matching note on.\n",
      "Warning: Note off for 51 on instrument 81 without matching note on.\n",
      "Warning: Note off for 81 on instrument 80 without matching note on.\n",
      "Warning: Note off for 37 on instrument 80 without matching note on.\n",
      "Warning: Note off for 59 on instrument 80 without matching note on.\n",
      "Warning: Note off for 84 on instrument 80 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 95 on instrument 81 without matching note on.\n",
      "Warning: Note off for 79 on instrument 80 without matching note on.\n",
      "Warning: Note off for 85 on instrument 81 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 94 on instrument 81 without matching note on.\n",
      "Warning: Note off for 65 on instrument 80 without matching note on.\n",
      "Warning: Note off for 81 on instrument 81 without matching note on.\n",
      "Warning: Note off for 41 on instrument 81 without matching note on.\n",
      "Warning: Note off for 63 on instrument 81 without matching note on.\n",
      "Warning: Note off for 80 on instrument 81 without matching note on.\n",
      "Warning: Note off for 49 on instrument 81 without matching note on.\n",
      "Warning: Note off for 71 on instrument 38 without matching note on.\n",
      "Warning: Note off for 78 on instrument 81 without matching note on.\n",
      "Warning: Note off for 43 on instrument 80 without matching note on.\n",
      "Warning: Note off for 63 on instrument 81 without matching note on.\n",
      "Warning: Note off for 78 on instrument 38 without matching note on.\n",
      "Warning: Note off for 53 on instrument 38 without matching note on.\n",
      "Warning: Note off for 43 on instrument 38 without matching note on.\n",
      "Warning: Note off for 70 on instrument 80 without matching note on.\n",
      "Warning: Note off for 55 on instrument 81 without matching note on.\n",
      "Warning: Note off for 55 on instrument 81 without matching note on.\n",
      "Warning: Note off for 52 on instrument 38 without matching note on.\n",
      "Warning: Note off for 85 on instrument 80 without matching note on.\n",
      "Warning: Note off for 84 on instrument 81 without matching note on.\n"
     ]
    }
   ],
   "source": [
    "start_token = VOCABULARY[BEGINNING_OF_SONG_TOKEN]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "generated_sequence = sample(model, start_token, max_length=1024, device=device)\n",
    "generated_tokens = [ID_TO_TOKEN[token] for token in generated_sequence]\n",
    "tokens_reconstructed = tokens_to_midi(generated_tokens)\n",
    "tokens_reconstructed.write('generated_midi.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1ad99-b7b6-4dc5-bbe8-20137acc85af",
   "metadata": {},
   "source": [
    "# Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59929515-8b46-474b-9b9e-6f33bfb6992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        max_seq_length=2048\n",
    "    ):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        # Transformer decoder layers (each has multi-head attention + MLP)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-LN for better stability\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Xavier uniform initialization\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generate a mask to prevent attention to future positions\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x, memory=None):\n",
    "        # x: (batch_size, seq_length)\n",
    "        batch_size, seq_length = x.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_embeddings = self.embedding(x) * (self.d_model ** 0.5)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(seq_length, device=x.device).unsqueeze(0)\n",
    "        pos_embeddings = self.pos_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_embeddings + pos_embeddings\n",
    "        \n",
    "        # Create causal mask\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_length).to(x.device)\n",
    "        \n",
    "        # Pass through transformer decoder\n",
    "        # In decoder-only mode, we use the embeddings as both tgt and memory\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=embeddings,\n",
    "            memory=embeddings,  # Self-attention only\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=tgt_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, None  # Return None for hidden state compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "711da2b7-8e85-4c85-a59c-34d06b056ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "E01 train | loss 3.2707 | ppl  26.33 | acc 0.319\n",
      "E01 val   | loss 3.0130 | ppl  20.35 | acc 0.302\n",
      "\n",
      "E02 train | loss 1.9959 | ppl   7.36 | acc 0.495\n",
      "E02 val   | loss 1.8509 | ppl   6.37 | acc 0.539\n",
      "\n",
      "E03 train | loss 1.3613 | ppl   3.90 | acc 0.638\n",
      "E03 val   | loss 1.5033 | ppl   4.50 | acc 0.620\n",
      "\n",
      "E04 train | loss 1.1574 | ppl   3.18 | acc 0.687\n",
      "E04 val   | loss 1.4033 | ppl   4.07 | acc 0.645\n",
      "\n",
      "E05 train | loss 1.0615 | ppl   2.89 | acc 0.710\n",
      "E05 val   | loss 1.3593 | ppl   3.89 | acc 0.656\n",
      "\n",
      "E06 train | loss 1.0003 | ppl   2.72 | acc 0.725\n",
      "E06 val   | loss 1.3314 | ppl   3.79 | acc 0.664\n",
      "\n",
      "E07 train | loss 0.9558 | ppl   2.60 | acc 0.735\n",
      "E07 val   | loss 1.3250 | ppl   3.76 | acc 0.667\n",
      "\n",
      "E08 train | loss 0.9206 | ppl   2.51 | acc 0.743\n",
      "E08 val   | loss 1.3136 | ppl   3.72 | acc 0.670\n",
      "\n",
      "E09 train | loss 0.8900 | ppl   2.44 | acc 0.750\n",
      "E09 val   | loss 1.3130 | ppl   3.72 | acc 0.671\n",
      "\n",
      "E10 train | loss 0.8662 | ppl   2.38 | acc 0.756\n",
      "E10 val   | loss 1.3236 | ppl   3.76 | acc 0.670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    \"\"\"Compute token-level accuracy.\"\"\"\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    correct = (preds == targets).sum().item()\n",
    "    total   = targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "def train(\n",
    "    model, train_loader, val_loader,\n",
    "    vocab_size, num_epochs=10, lr=1e-3, device='cuda'):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---------- Training ----------\n",
    "        model.train()\n",
    "        tr_loss = tr_tokens = tr_correct = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)              # (B, T, V)\n",
    "            loss = criterion(logits.view(-1, vocab_size),\n",
    "                             targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss     += loss.item()\n",
    "            tr_correct  += (logits.argmax(-1) == targets).sum().item()\n",
    "            tr_tokens   += targets.numel()\n",
    "\n",
    "        avg_tr_loss = tr_loss / len(train_loader)\n",
    "        tr_ppl      = math.exp(avg_tr_loss)\n",
    "        tr_acc      = tr_correct / tr_tokens\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        model.eval()\n",
    "        val_loss = val_tokens = val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits.view(-1, vocab_size),\n",
    "                                 targets.view(-1))\n",
    "                val_loss    += loss.item()\n",
    "                val_correct += (logits.argmax(-1) == targets).sum().item()\n",
    "                val_tokens  += targets.numel()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_ppl      = math.exp(avg_val_loss)\n",
    "        val_acc      = val_correct / val_tokens\n",
    "\n",
    "        print(f\"E{epoch+1:02d} train | loss {avg_tr_loss:.4f} | \"\n",
    "              f\"ppl {tr_ppl:6.2f} | acc {tr_acc:.3f}\")\n",
    "        print(f\"E{epoch+1:02d} val   | loss {avg_val_loss:.4f} | \"\n",
    "              f\"ppl {val_ppl:6.2f} | acc {val_acc:.3f}\\n\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Model initialization with correct parameters\n",
    "batch_size = 32  # You can adjust this depending on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize MusicTransformer with correct parameters\n",
    "model = MusicTransformer(\n",
    "    vocab_size=len(VOCABULARY),\n",
    "    d_model=512,           # Model dimension (replaces embedding_dim)\n",
    "    nhead=8,               # Number of attention heads\n",
    "    num_layers=3,          # Number of transformer layers\n",
    "    dim_feedforward=2048,  # MLP hidden dimension\n",
    "    dropout=0.1,           # Dropout rate\n",
    "    max_seq_length=512    # Maximum sequence length\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, train_loader, val_loader, vocab_size=len(VOCABULARY), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75622a7-52fa-43df-a937-15e1f592c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Note off for 64 on instrument 80 without matching note on.\n",
      "Warning: Note off for 70 on instrument 80 without matching note on.\n",
      "Warning: Note off for 74 on instrument 81 without matching note on.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample(\n",
    "    model,\n",
    "    start_token: int,\n",
    "    max_length: 512,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressively sample from a trained MusicTransformer.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : nn.Module\n",
    "        Trained MusicTransformer instance.\n",
    "    start_token : int\n",
    "        Vocabulary id that marks the beginning-of-song.\n",
    "    max_length : int\n",
    "        Maximum number of tokens (including the start_token).\n",
    "    temperature : float\n",
    "        Softmax temperature (>1 = more random, <1 = more greedy).\n",
    "    device : str\n",
    "        'cuda', 'mps', or 'cpu'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int] : generated token ids (including the start_token).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # running sequence buffer (1, t)\n",
    "    generated = [start_token]\n",
    "    input_tokens = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - 1):          # we already have one token\n",
    "            logits, _ = model(input_tokens)      # (1, t, vocab)\n",
    "            logits = logits[:, -1, :]            # last step (1, vocab)\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)    # (1, vocab)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # stop if EOS or PAD\n",
    "            if next_token in (\n",
    "                VOCABULARY[END_OF_SONG_TOKEN],\n",
    "                VOCABULARY[PAD_TOKEN],\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # append and continue\n",
    "            input_tokens = torch.tensor(\n",
    "                [generated], dtype=torch.long, device=device\n",
    "            )\n",
    "\n",
    "    return generated\n",
    "start_token = VOCABULARY[BEGINNING_OF_SONG_TOKEN]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "generated_ids = sample(trained_model, start_token, max_length=512, device=device)\n",
    "generated_tokens = [ID_TO_TOKEN[i] for i in generated_ids]\n",
    "\n",
    "midi_obj = tokens_to_midi(generated_tokens)\n",
    "midi_obj.write('generated_midi2.mid')\n",
    "\n",
    "generated_ids = sample(trained_model, start_token, max_length=512, device=device)\n",
    "generated_tokens = [ID_TO_TOKEN[i] for i in generated_ids]\n",
    "\n",
    "midi_obj = tokens_to_midi(generated_tokens)\n",
    "midi_obj.write('generated_midi3.mid')\n",
    "\n",
    "\n",
    "generated_ids = sample(trained_model, start_token, max_length=512, device=device)\n",
    "generated_tokens = [ID_TO_TOKEN[i] for i in generated_ids]\n",
    "\n",
    "midi_obj = tokens_to_midi(generated_tokens)\n",
    "midi_obj.write('generated_midi4.mid')\n",
    "\n",
    "\n",
    "generated_ids = sample(trained_model, start_token, max_length=512, device=device)\n",
    "generated_tokens = [ID_TO_TOKEN[i] for i in generated_ids]\n",
    "\n",
    "midi_obj = tokens_to_midi(generated_tokens)\n",
    "midi_obj.write('generated_midi5.mid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab520814",
   "metadata": {},
   "source": [
    "### Train Soloists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2edbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_soloist_midi_files(filepaths, instrument_program):\n",
    "    valid_files = []\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        try:\n",
    "            midi_data = pretty_midi.PrettyMIDI(filepath)\n",
    "            if len(midi_data.instruments) > 0 and any(instrument.program == instrument_program for instrument in midi_data.instruments):\n",
    "                valid_files.append(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "        print_progress_bar(i+1, len(filepaths), prefix='Validating MIDI files')\n",
    "    return valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7eeb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instruments = [80, 81, 38, 121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633e8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Validating MIDI files |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "soloist_80_midi_train_filepaths = valid_soloist_midi_files(midi_train_filepaths, instrument_program=80)\n",
    "soloist_81_midi_train_filepaths = valid_soloist_midi_files(midi_train_filepaths, instrument_program=81)\n",
    "soloist_38_midi_train_filepaths = valid_soloist_midi_files(midi_train_filepaths, instrument_program=38)\n",
    "soloist_121_midi_train_filepaths = valid_soloist_midi_files(midi_train_filepaths, instrument_program=121)\n",
    "\n",
    "soloist_80_midi_val_filepaths = valid_soloist_midi_files(midi_val_filepaths, instrument_program=80)\n",
    "soloist_81_midi_val_filepaths = valid_soloist_midi_files(midi_val_filepaths, instrument_program=81)\n",
    "soloist_38_midi_val_filepaths = valid_soloist_midi_files(midi_val_filepaths, instrument_program=38)\n",
    "soloist_121_midi_val_filepaths = valid_soloist_midi_files(midi_val_filepaths, instrument_program=121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a6b66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soloist_vocabulary():\n",
    "    vocabulary = dict()\n",
    "    index = 0\n",
    "\n",
    "    for special_token in [PAD_TOKEN, BEGINNING_OF_SONG_TOKEN, END_OF_SONG_TOKEN]:\n",
    "        vocabulary[special_token] = index\n",
    "        index += 1\n",
    "\n",
    "    for time_shift in range(1, MAX_SHIFT_STEPS + 1):\n",
    "        vocabulary[f'time_shift_{time_shift}'] = index\n",
    "        index += 1\n",
    "\n",
    "    for action in [\"note_on\", \"note_off\"]:\n",
    "        for pitch in range(128):\n",
    "            vocabulary[f'{action}_{pitch}'] = index\n",
    "            index += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4f9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soloist_id_to_token():\n",
    "    vocabulary = get_soloist_vocabulary()\n",
    "    return {v: k for k, v in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e47dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soloist_midi_to_tokens(pm: pretty_midi.PrettyMIDI, instrument_program):\n",
    "    events = []\n",
    "\n",
    "    for instrument in pm.instruments:\n",
    "        if instrument.program == instrument_program:\n",
    "            for note in instrument.notes:\n",
    "                events.append((note.start, f'note_on_{note.pitch}'))\n",
    "                events.append((note.end, f'note_off_{note.pitch}'))\n",
    "        \n",
    "            break\n",
    "        \n",
    "    \n",
    "    events.sort()  # Sort by time\n",
    "\n",
    "    tokens = []\n",
    "    last_time = 0.0\n",
    "    for time, event in events:\n",
    "        delta = time - last_time\n",
    "        steps = max(round(delta / TIME_SHIFT_RESOLUTION), 1) # force at least 1 step (no side by side notes)\n",
    "\n",
    "        while steps > 0:\n",
    "            shift = min(steps, MAX_SHIFT_STEPS)\n",
    "            tokens.append(f'time_shift_{shift}')\n",
    "            steps -= shift\n",
    "        \n",
    "        tokens.append(event)\n",
    "        last_time = time\n",
    "    return [BEGINNING_OF_SONG_TOKEN] + tokens + [END_OF_SONG_TOKEN]\n",
    "\n",
    "def soloist_tokens_to_midi(tokens, instrument_program):\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=instrument_program)\n",
    "\n",
    "    active_notes = dict()\n",
    "    active_pitch = None\n",
    "    active_start = None\n",
    "\n",
    "    current_time = 0.0\n",
    "    for token in tokens:\n",
    "        # print(current_time, token)\n",
    "        if token.startswith('time_shift_'):\n",
    "            shift_steps = int(token.split('_')[-1])\n",
    "            current_time += shift_steps * TIME_SHIFT_RESOLUTION\n",
    "        elif token.startswith('note_on_'):\n",
    "            if active_pitch is not None:\n",
    "                print(f\"Warning: Skipping {token}, other note: {active_pitch} is still active.\")\n",
    "                continue \n",
    "            pitch = int(token.split('_')[2])\n",
    "            # active_notes[pitch] = current_time\n",
    "            active_pitch = pitch\n",
    "            active_start = current_time\n",
    "        elif token.startswith('note_off_'):\n",
    "            pitch = int(token.split('_')[2])\n",
    "            if pitch != active_pitch:\n",
    "                print(f\"Warning: Note off for {pitch} without matching note on.\")\n",
    "                continue\n",
    "\n",
    "            if current_time > active_start:\n",
    "                note = pretty_midi.Note(\n",
    "                    velocity=100, pitch=pitch, start=active_start, end=current_time\n",
    "                )\n",
    "\n",
    "                instrument.notes.append(note)\n",
    "            else:\n",
    "                print(f\"Warning: Note off for {pitch} at {current_time} note after note on at {active_start}. Ignoring.\")\n",
    "            # del active_notes[pitch]\n",
    "            active_pitch = None\n",
    "            active_start = None\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    \n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90aef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = pretty_midi.PrettyMIDI(all_filepaths[0])\n",
    "tokens_80 = soloist_midi_to_tokens(midi, 80)\n",
    "tokens_81 = soloist_midi_to_tokens(midi, 81)\n",
    "tokens_38 = soloist_midi_to_tokens(midi, 38)\n",
    "tokens_121 = soloist_midi_to_tokens(midi, 121)\n",
    "\n",
    "midi_reconstructed_80 = soloist_tokens_to_midi(tokens_80, instrument_program=80)\n",
    "midi_reconstructed_81 = soloist_tokens_to_midi(tokens_81, instrument_program=81)\n",
    "midi_reconstructed_38 = soloist_tokens_to_midi(tokens_38, instrument_program=38)\n",
    "midi_reconstructed_121 = soloist_tokens_to_midi(tokens_121, instrument_program=121)\n",
    "\n",
    "# midi_reconstructed_80.write('reconstructed_midi_80.mid')\n",
    "# midi_reconstructed_81.write('reconstructed_midi_81.mid')\n",
    "# midi_reconstructed_38.write('reconstructed_midi_38.mid')\n",
    "# midi_reconstructed_121.write('reconstructed_midi_121.mid')\n",
    "\n",
    "# midi.write('original_midi.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c3dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soloist_sequences(filepaths, instrument_program):\n",
    "    vocabulary = get_soloist_vocabulary()\n",
    "    sequences = []\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        pm = pretty_midi.PrettyMIDI(filepath)\n",
    "        tokens = soloist_midi_to_tokens(pm, instrument_program)\n",
    "        if not tokens:\n",
    "            raise ValueError(f'No tokens generated for {filepath}')\n",
    "        sequences.append([vocabulary[token] for token in tokens])\n",
    "        print_progress_bar(i+1, len(filepaths), prefix='Loading sequences')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38e9e573",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soloist_80_midi_train_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m soloist_80_train_sequences \u001b[38;5;241m=\u001b[39m load_soloist_sequences(\u001b[43msoloist_80_midi_train_filepaths\u001b[49m, instrument_program\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      2\u001b[0m soloist_81_train_sequences \u001b[38;5;241m=\u001b[39m load_soloist_sequences(soloist_81_midi_train_filepaths, instrument_program\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m81\u001b[39m)\n\u001b[1;32m      3\u001b[0m soloist_38_train_sequences \u001b[38;5;241m=\u001b[39m load_soloist_sequences(soloist_38_midi_train_filepaths, instrument_program\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m38\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soloist_80_midi_train_filepaths' is not defined"
     ]
    }
   ],
   "source": [
    "soloist_80_train_sequences = load_soloist_sequences(soloist_80_midi_train_filepaths, instrument_program=80)\n",
    "soloist_81_train_sequences = load_soloist_sequences(soloist_81_midi_train_filepaths, instrument_program=81)\n",
    "soloist_38_train_sequences = load_soloist_sequences(soloist_38_midi_train_filepaths, instrument_program=38)\n",
    "soloist_121_train_sequences = load_soloist_sequences(soloist_121_midi_train_filepaths, instrument_program=121)\n",
    "\n",
    "soloist_80_val_sequences = load_soloist_sequences(soloist_80_midi_val_filepaths, instrument_program=80)\n",
    "soloist_81_val_sequences = load_soloist_sequences(soloist_81_midi_val_filepaths, instrument_program=81)\n",
    "soloist_38_val_sequences = load_soloist_sequences(soloist_38_midi_val_filepaths, instrument_program=38)\n",
    "soloist_121_val_sequences = load_soloist_sequences(soloist_121_midi_val_filepaths, instrument_program=121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soloist_model():\n",
    "    return MusicRNN(vocab_size=len(get_soloist_vocabulary()), embedding_dim=64, hidden_dim=512, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bea435-4985-42c0-90fb-0300ea41ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soloist_model_transformer():\n",
    "    return  MusicTransformer(\n",
    "    vocab_size=len(get_soloist_vocabulary()),\n",
    "    d_model=256,           # Model dimension (replaces embedding_dim)\n",
    "    nhead=8,               # Number of attention heads\n",
    "    num_layers=2,          # Number of transformer layers\n",
    "    dim_feedforward=2048,  # MLP hidden dimension\n",
    "    dropout=0.1,           # Dropout rate\n",
    "    max_seq_length=512    # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa97d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_soloists(soloist_sequences: dict):\n",
    "    models = {}\n",
    "    for instrument_program, (train_sequences, val_sequences) in soloist_sequences.items():\n",
    "        train_dataset = MIDITokenDataset(train_sequences, seq_length=512)\n",
    "        val_dataset = MIDITokenDataset(val_sequences, seq_length=512)\n",
    "\n",
    "        batch_size = 8\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = get_soloist_model()\n",
    "        train(model, train_loader, val_loader, vocab_size=len(get_soloist_vocabulary()), device=device)\n",
    "        models[instrument_program] = model\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1d48761-e91e-42ac-9270-d6c85bcda021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_soloists_transformer(soloist_sequences: dict):\n",
    "    models = {}\n",
    "    for instrument_program, (train_sequences, val_sequences) in soloist_sequences.items():\n",
    "        train_dataset = MIDITokenDataset(train_sequences, seq_length=512)\n",
    "        val_dataset = MIDITokenDataset(val_sequences, seq_length=512)\n",
    "\n",
    "        batch_size = 8\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = get_soloist_model_transformer()\n",
    "        train(model, train_loader, val_loader, vocab_size=len(get_soloist_vocabulary()), device=device)\n",
    "        models[instrument_program] = model\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ffb7c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soloist_80_train_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m soloist_sequences \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;241m80\u001b[39m: (\u001b[43msoloist_80_train_sequences\u001b[49m, soloist_80_val_sequences),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m81\u001b[39m: (soloist_81_train_sequences, soloist_81_val_sequences),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m38\u001b[39m: (soloist_38_train_sequences, soloist_38_val_sequences),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m121\u001b[39m: (soloist_121_train_sequences, soloist_121_val_sequences)\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m soloist_models \u001b[38;5;241m=\u001b[39m train_soloists(soloist_sequences)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soloist_80_train_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "soloist_sequences = {\n",
    "    80: (soloist_80_train_sequences, soloist_80_val_sequences),\n",
    "    81: (soloist_81_train_sequences, soloist_81_val_sequences),\n",
    "    38: (soloist_38_train_sequences, soloist_38_val_sequences),\n",
    "    121: (soloist_121_train_sequences, soloist_121_val_sequences)\n",
    "}\n",
    "soloist_models = train_soloists(soloist_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74b7c36-a1f3-4bb5-991a-cddabca3434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |███████████████████████████████████████████-------| 87.9% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 1.6864 | ppl   5.40 | acc 0.580\n",
      "E01 val   | loss 1.0829 | ppl   2.95 | acc 0.702\n",
      "\n",
      "E02 train | loss 0.9695 | ppl   2.64 | acc 0.725\n",
      "E02 val   | loss 0.9542 | ppl   2.60 | acc 0.737\n",
      "\n",
      "E03 train | loss 0.8705 | ppl   2.39 | acc 0.751\n",
      "E03 val   | loss 0.9450 | ppl   2.57 | acc 0.742\n",
      "\n",
      "E04 train | loss 0.8233 | ppl   2.28 | acc 0.762\n",
      "E04 val   | loss 0.9467 | ppl   2.58 | acc 0.741\n",
      "\n",
      "E05 train | loss 0.7884 | ppl   2.20 | acc 0.771\n",
      "E05 val   | loss 0.9164 | ppl   2.50 | acc 0.750\n",
      "\n",
      "E06 train | loss 0.7572 | ppl   2.13 | acc 0.780\n",
      "E06 val   | loss 0.8804 | ppl   2.41 | acc 0.759\n",
      "\n",
      "E07 train | loss 0.7263 | ppl   2.07 | acc 0.789\n",
      "E07 val   | loss 0.8210 | ppl   2.27 | acc 0.781\n",
      "\n",
      "E08 train | loss 0.6853 | ppl   1.98 | acc 0.802\n",
      "E08 val   | loss 0.7839 | ppl   2.19 | acc 0.794\n",
      "\n",
      "E09 train | loss 0.6584 | ppl   1.93 | acc 0.810\n",
      "E09 val   | loss 0.7784 | ppl   2.18 | acc 0.797\n",
      "\n",
      "E10 train | loss 0.6393 | ppl   1.90 | acc 0.815\n",
      "E10 val   | loss 0.7582 | ppl   2.13 | acc 0.803\n",
      "\n",
      "Processing sequences |███████████████████████████████████---------------| 70.2% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 1.6761 | ppl   5.34 | acc 0.580\n",
      "E01 val   | loss 1.0120 | ppl   2.75 | acc 0.724\n",
      "\n",
      "E02 train | loss 0.9537 | ppl   2.60 | acc 0.728\n",
      "E02 val   | loss 0.9039 | ppl   2.47 | acc 0.754\n",
      "\n",
      "E03 train | loss 0.8656 | ppl   2.38 | acc 0.751\n",
      "E03 val   | loss 0.8346 | ppl   2.30 | acc 0.776\n",
      "\n",
      "E04 train | loss 0.8184 | ppl   2.27 | acc 0.763\n",
      "E04 val   | loss 0.8566 | ppl   2.36 | acc 0.767\n",
      "\n",
      "E05 train | loss 0.7857 | ppl   2.19 | acc 0.772\n",
      "E05 val   | loss 0.8168 | ppl   2.26 | acc 0.780\n",
      "\n",
      "E06 train | loss 0.7454 | ppl   2.11 | acc 0.785\n",
      "E06 val   | loss 0.7439 | ppl   2.10 | acc 0.803\n",
      "\n",
      "E07 train | loss 0.6999 | ppl   2.01 | acc 0.800\n",
      "E07 val   | loss 0.7053 | ppl   2.02 | acc 0.815\n",
      "\n",
      "E08 train | loss 0.6757 | ppl   1.97 | acc 0.807\n",
      "E08 val   | loss 0.6977 | ppl   2.01 | acc 0.815\n",
      "\n",
      "E09 train | loss 0.6522 | ppl   1.92 | acc 0.813\n",
      "E09 val   | loss 0.6717 | ppl   1.96 | acc 0.823\n",
      "\n",
      "E10 train | loss 0.6366 | ppl   1.89 | acc 0.817\n",
      "E10 val   | loss 0.6697 | ppl   1.95 | acc 0.825\n",
      "\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 1.7276 | ppl   5.63 | acc 0.563\n",
      "E01 val   | loss 0.9831 | ppl   2.67 | acc 0.715\n",
      "\n",
      "E02 train | loss 0.9008 | ppl   2.46 | acc 0.744\n",
      "E02 val   | loss 0.8177 | ppl   2.27 | acc 0.767\n",
      "\n",
      "E03 train | loss 0.7988 | ppl   2.22 | acc 0.769\n",
      "E03 val   | loss 0.7962 | ppl   2.22 | acc 0.770\n",
      "\n",
      "E04 train | loss 0.7560 | ppl   2.13 | acc 0.779\n",
      "E04 val   | loss 0.7860 | ppl   2.19 | acc 0.776\n",
      "\n",
      "E05 train | loss 0.7251 | ppl   2.06 | acc 0.787\n",
      "E05 val   | loss 0.7825 | ppl   2.19 | acc 0.776\n",
      "\n",
      "E06 train | loss 0.7038 | ppl   2.02 | acc 0.793\n",
      "E06 val   | loss 0.7876 | ppl   2.20 | acc 0.778\n",
      "\n",
      "E07 train | loss 0.6801 | ppl   1.97 | acc 0.798\n",
      "E07 val   | loss 0.7666 | ppl   2.15 | acc 0.783\n",
      "\n",
      "E08 train | loss 0.6610 | ppl   1.94 | acc 0.804\n",
      "E08 val   | loss 0.7779 | ppl   2.18 | acc 0.783\n",
      "\n",
      "E09 train | loss 0.6422 | ppl   1.90 | acc 0.809\n",
      "E09 val   | loss 0.7756 | ppl   2.17 | acc 0.783\n",
      "\n",
      "E10 train | loss 0.6181 | ppl   1.86 | acc 0.817\n",
      "E10 val   | loss 0.7308 | ppl   2.08 | acc 0.798\n",
      "\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 1.2824 | ppl   3.61 | acc 0.600\n",
      "E01 val   | loss 0.9199 | ppl   2.51 | acc 0.700\n",
      "\n",
      "E02 train | loss 0.5588 | ppl   1.75 | acc 0.817\n",
      "E02 val   | loss 0.6987 | ppl   2.01 | acc 0.783\n",
      "\n",
      "E03 train | loss 0.4329 | ppl   1.54 | acc 0.858\n",
      "E03 val   | loss 0.6969 | ppl   2.01 | acc 0.792\n",
      "\n",
      "E04 train | loss 0.3959 | ppl   1.49 | acc 0.869\n",
      "E04 val   | loss 0.7297 | ppl   2.07 | acc 0.792\n",
      "\n",
      "E05 train | loss 0.3741 | ppl   1.45 | acc 0.876\n",
      "E05 val   | loss 0.7183 | ppl   2.05 | acc 0.793\n",
      "\n",
      "E06 train | loss 0.3549 | ppl   1.43 | acc 0.882\n",
      "E06 val   | loss 0.6892 | ppl   1.99 | acc 0.800\n",
      "\n",
      "E07 train | loss 0.3378 | ppl   1.40 | acc 0.888\n",
      "E07 val   | loss 0.6249 | ppl   1.87 | acc 0.814\n",
      "\n",
      "E08 train | loss 0.3221 | ppl   1.38 | acc 0.893\n",
      "E08 val   | loss 0.5925 | ppl   1.81 | acc 0.824\n",
      "\n",
      "E09 train | loss 0.3099 | ppl   1.36 | acc 0.896\n",
      "E09 val   | loss 0.5853 | ppl   1.80 | acc 0.824\n",
      "\n",
      "E10 train | loss 0.3018 | ppl   1.35 | acc 0.899\n",
      "E10 val   | loss 0.6088 | ppl   1.84 | acc 0.822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soloist_sequences = {\n",
    "    80: (soloist_80_train_sequences, soloist_80_val_sequences),\n",
    "    81: (soloist_81_train_sequences, soloist_81_val_sequences),\n",
    "    38: (soloist_38_train_sequences, soloist_38_val_sequences),\n",
    "    121: (soloist_121_train_sequences, soloist_121_val_sequences)\n",
    "}\n",
    "soloist_models = train_soloists_transformer(soloist_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "619737a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(models):\n",
    "    for instrument_program, model in models.items():\n",
    "        torch.save(model.state_dict(), f'soloist_model_{instrument_program}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c1943b5-1dd7-4526-9a29-adcc72afdf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicTransformer(\n",
      "  (embedding): Embedding(359, 256)\n",
      "  (pos_embedding): Embedding(512, 256)\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=359, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(soloist_models[\u001b[38;5;241m80\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoloist_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoloist_model_80kio.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/serialization.py:965\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 965\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/serialization.py:1264\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         storage \u001b[38;5;241m=\u001b[39m new_storage\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m         storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/storage.py:262\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(soloist_models[80])\n",
    "torch.save(soloist_models[80].state_dict(), f'soloist_model_80kio.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9592015",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_model_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoloist_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m, in \u001b[0;36msave_model_weights\u001b[0;34m(models)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_model_weights\u001b[39m(models):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m instrument_program, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoloist_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minstrument_program\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/serialization.py:965\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 965\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/serialization.py:1264\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         storage \u001b[38;5;241m=\u001b[39m new_storage\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m         storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/storage.py:262\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "save_model_weights(soloist_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bdb8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weight(path, model):\n",
    "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_soloists(models, outdir='./'):\n",
    "    soloist_vocabulary = get_soloist_vocabulary()\n",
    "    midis = dict()\n",
    "    for instrument_program, model in models.items():\n",
    "        model.eval()  # Set to eval mode for inference\n",
    "        start_token = soloist_vocabulary[BEGINNING_OF_SONG_TOKEN]\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        generated_sequence = sample(model, start_token, max_length=1024, device=device)\n",
    "        generated_tokens = [get_soloist_id_to_token()[token] for token in generated_sequence]\n",
    "        tokens_reconstructed = soloist_tokens_to_midi(generated_tokens, instrument_program=instrument_program)\n",
    "        midis[instrument_program] = tokens_reconstructed\n",
    "    \n",
    "    together_midi = pretty_midi.PrettyMIDI()\n",
    "    for instrument_program, midi in midis.items():\n",
    "        together_midi.instruments.extend(midi.instruments)\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = os.path.join(outdir, f'{datetime.datetime.now()}.mid')\n",
    "    \n",
    "    together_midi.write(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016d3584-8940-4538-b101-b680d2d957fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict     # <- add this\n",
    "import os, datetime, torch, pretty_midi\n",
    "from typing import Dict\n",
    "def sample(\n",
    "    model,\n",
    "    start_token: int,\n",
    "    vocab: Dict[str, int],\n",
    "    max_length: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"Autoregressive sampling for our MusicTransformer variant.\"\"\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    eos_id = vocab[END_OF_SONG_TOKEN]\n",
    "    pad_id = vocab[PAD_TOKEN]\n",
    "\n",
    "    generated = [start_token]\n",
    "    tokens = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - 1):\n",
    "            logits, _ = model(tokens)          # (1, T, |V|)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs  = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            next_tok = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_tok)\n",
    "\n",
    "            if next_tok in (eos_id, pad_id):\n",
    "                break\n",
    "\n",
    "            tokens = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n",
    "    \n",
    "def sample_soloists(\n",
    "    models: Dict[int, torch.nn.Module],\n",
    "    outdir: str = \"./\",\n",
    "    max_length: int = 512,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a track for each instrument-program in `models`\n",
    "    and merge them into one multitrack MIDI file.\n",
    "    \"\"\"\n",
    "    device = (\n",
    "        device\n",
    "        or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "    vocab          = get_soloist_vocabulary()\n",
    "    id_to_token    = get_soloist_id_to_token()\n",
    "    midis          = OrderedDict()\n",
    "\n",
    "    # --- generate each solo line ------------------------------------------------\n",
    "    for prog, model in models.items():\n",
    "        start_tok   = vocab[BEGINNING_OF_SONG_TOKEN]\n",
    "        token_ids   = sample(\n",
    "            model,\n",
    "            start_tok,\n",
    "            vocab=vocab,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            device=device,\n",
    "        )\n",
    "        tokens      = [id_to_token[i] for i in token_ids]\n",
    "        midi_track  = soloist_tokens_to_midi(tokens, instrument_program=prog)\n",
    "        midis[prog] = midi_track\n",
    "\n",
    "    # --- merge into a single PrettyMIDI ----------------------------------------\n",
    "    together = pretty_midi.PrettyMIDI()\n",
    "    for midi in midis.values():\n",
    "        together.instruments.extend(midi.instruments)\n",
    "\n",
    "    # --- make sure output dir exists and save ----------------------------------\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    outpath   = os.path.join(outdir, f\"soloists_{timestamp}.mid\")\n",
    "\n",
    "    together.write(outpath)\n",
    "    print(f\"✨  Wrote multitrack MIDI to: {outpath}\")\n",
    "\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cce8625-9201-4da6-8841-036d9a79f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instruments = [38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66c021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨  Wrote multitrack MIDI to: lstm_models/soloists/2/samples/soloists_20250602_034729.mid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lstm_models/soloists/2/samples/soloists_20250602_034729.mid'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models = [load_model_weight(f'soloist_model_{instrument_program}.pth', get_soloist_model()) for instrument_program in unique_instruments]\n",
    "model_weights_dir = os.path.join('lstm_models', 'soloists', '2')\n",
    "# models = {80: load_model_weight(os.path.join(model_weights_dir, 'soloist_model_80.pth'), get_soloist_model())}\n",
    "models = {instr_program: load_model_weight( f'soloist_model_{instr_program}.pth', get_soloist_model_transformer()) for instr_program in unique_instruments}\n",
    "sample_soloists(models, outdir=os.path.join(model_weights_dir, 'samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_note_count = 0\n",
    "total_tokens = 0\n",
    "for i, sequence in enumerate(soloist_80_train_sequences):\n",
    "    pairwise = zip(sequence[:-1], sequence[1:])\n",
    "    total_tokens += len(sequence)\n",
    "    for token1, token2 in pairwise:\n",
    "        token1 = get_soloist_id_to_token()[token1]\n",
    "        token2 = get_soloist_id_to_token()[token2]\n",
    "        if token1.startswith('note_on_') and token2.startswith('note_off_'):\n",
    "            if token1.split('_')[2] == token2.split('_')[2]:\n",
    "                side_by_side_note_count += 1\n",
    "\n",
    "    print_progress_bar(i+1, len(soloist_80_train_sequences), prefix='Counting side by side notes')\n",
    "    \n",
    "print(f'Side by side note count: {side_by_side_note_count}')\n",
    "print(f'Total tokens: {total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7125c7",
   "metadata": {},
   "source": [
    "### Task 2 - Conditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd8e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game name distribution: {'10_YardFight': 2, '1942': 6, '720_': 4, '98in1': 1, 'Abadox_TheDeadlyInnerWar': 12, 'Adam_amp_Eve': 5, 'AfterBurner': 5, 'AfterBurnerII': 6, 'AighinanoYogen_BalubalouknoDensetsuYori': 14, 'AlienSyndrome': 10, 'Aliens_Alien2': 9, 'Argus': 10, 'ArmWrestling': 1, 'ArumananoKiseki': 12, 'Athena': 13, 'AtlantisnoNazo': 7, 'BabelnoTou': 8, 'BalloonFight': 12, 'Baseball': 6, 'BatmanReturns': 20, 'BinaryLand': 6, 'Batman_ReturnofTheJoker': 15, 'Batman_TheVideoGame': 11, 'BattleCity': 3, 'BioMiracleBokutteUpa': 13, 'BioSenshiDan_IncreasertonoTatakai': 24, 'Blackjack': 3, 'BlasterMaster': 14, 'Bomberman': 10, 'BombermanII': 18, 'BuraiFighter': 9, 'CaptainTsubasaVol_II_SuperStriker': 40, 'Castelian': 5, 'CastleofDragon': 16, 'Castlevania': 16, 'CastlevaniaIII_Dracula_sCurse': 28, 'CastlevaniaII_Simon_sQuest': 9, 'Chack_nPop': 5, 'Challenger': 6, 'ChaosWorld': 24, 'ChesterField_EpisodeIIAnkokuShinenoChousen': 19, 'ChoujinSentaiJetman': 18, 'CircusCaper': 26, 'CircusCharlie': 4, 'CluCluLand': 7, 'Contra': 13, 'ContraForce': 14, 'CrisisForce': 12, 'DarkLord': 27, 'DeadlyTowers': 26, 'Deathbots': 5, 'DeepDungeonII_YuushinoMonshou': 16, 'DefenderII': 3, 'DestinyofanEmperor': 20, 'Dezaemon': 7, 'DigDug': 9, 'DigDugII': 10, 'DonDokoDon': 12, 'DonkeyKong': 13, 'DonkeyKong3': 9, 'DonkeyKongCountry4': 13, 'DonkeyKongJr_': 20, 'Doraemon': 19, 'DoubleDribble': 8, 'Dr_JekyllandMr_Hyde': 18, 'Dr_Mario': 11, 'DraculaII_NoroinoFuuin': 9, 'DragonBuster': 11, 'DragonFighter': 13, 'DragonSpirit_TheNewLegend': 19, 'DragonWarrior': 25, 'DragonWarriorII': 25, 'DragonWarriorIII': 33, 'DragonWarriorIV': 46, 'DuckTales': 13, 'Excitebike': 6, 'ElevatorAction': 1, 'ExcitingBasket': 5, 'ExedExes': 7, 'F_1Race': 3, 'Falsion': 10, 'FamicomBox': 2, 'FamicomGrandPrixII_3DHotRally': 10, 'FamicomJumpII_Saikyono7_nin': 19, 'FamicomJump_HeroRetsuden': 36, 'GetsuFuumaDen': 15, 'FamicomMukashiBanashi_ShinOnigashima': 25, 'Famista_90': 13, 'Famista_91': 22, 'Famista_93': 20, 'FantasyZone': 14, 'FantasyZoneII_TheTeardropofOpa_Opa': 14, 'FelixtheCat': 22, 'FinalFantasy': 20, 'FinalFantasyII': 24, 'FinalFantasyIII': 59, 'FinalMission': 12, 'FireEmblemGaiden': 34, 'FireEmblem_AnkokuRyutoHikarinoTsurugi': 30, 'Fire_nIce': 24, 'FlyingDragon_TheSecretScroll': 13, 'FlyingHero': 10, 'FlyingWarriors': 30, 'FreedomForce': 18, 'Fridaythe13th': 5, 'Galaga': 10, 'GallForce_EternalStory': 4, 'GanbareGoemon2': 20, 'GanbareGoemonGaiden2_TenkanoZaih_': 99, 'GanbareGoemon_KarakuriDouchuu': 18, 'GansoSaiyuuki_SuperMonkeyDaibouken': 5, 'Gargoyle_sQuestII': 21, 'GenpeiToumaden_ComputerBoardgame': 21, 'Ghosts_nGoblins': 14, 'GingaDenshou_GalaxyOdyssey': 11, 'GomokuNarabeRenju': 7, 'Gradius': 12, 'GradiusII': 15, 'GreenBeret': 22, 'Gumshoe': 14, 'Gyromite': 11, 'HappilyEverAfter': 11, 'HeraclesnoEikouII_TitannoMetsubou': 19, 'IkariWarriors': 5, 'HigemaruMakaijima_NanatsunoShimaDaibouken': 16, 'HikariShinwa_PalutenanoKagami': 12, 'HinoToriHououhen_GaounoBouken': 11, 'HiryunoKenIII_5NinnoRyuSenshi': 26, 'HiryunoKenII_DragonnoTsubasa': 32, 'HiryunoKenSpecial_FightingWars': 17, 'HittheIce': 15, 'HolyDiver': 18, 'Hudson_sAdventureIsland': 14, 'IceClimber': 4, 'IkariIII_TheRescue': 16, 'IkariWarriorsII_VictoryRoad': 7, 'ImageFight': 18, 'IndianaJonesandtheTempleofDoom': 4, 'Ironsword_Wizards_amp_WarriorsII': 16, 'JourneytoSilius': 12, 'KainoBouken_TheQuestofKi': 13, 'Joust': 1, 'KiwiKraze': 10, 'KekkyokuNankyokuDaibouken': 4, 'KickMaster': 24, 'KidIcarus': 12, 'KidNiki_RadicalNinja': 10, 'KidouSenshiZ_Gundam_HotScramble': 5, 'KingKong2_IkarinoMegatonPunch': 13, 'King_sKnight': 9, 'Kirby_sAdventure': 42, 'Klax': 5, 'KonamiWaiWaiWorld': 21, 'KujakuOu': 12, 'KujakuOuII': 13, 'Labyrinth_MaounoMeikyu': 18, 'LanMaster': 3, 'MegaMan4': 30, 'LawnMower': 5, 'LegacyofTheWizard': 16, 'Lemmings': 10, 'LesChevaliersduZodiaque_LaLegended_Or': 9, 'LifeForce': 11, 'LowGMan_TheLowGravityMan': 15, 'M82GameSelectableWorkingProductDisplay': 1, 'MachRider': 11, 'MTV_RemoteControl': 1, 'Magician': 14, 'Mahjong': 6, 'MajouDensetsuII_DaimashikyouGalious': 14, 'Mappy': 7, 'MarbleMadness': 8, 'MarioBros_': 6, 'MarusanoOnna': 10, 'Maxi15': 1, 'MegaMan': 16, 'MegaMan2': 26, 'MegaMan3': 24, 'MegaMan5': 27, 'MegaMan6': 36, 'Metroid': 12, 'Meiky_JiinDababa': 13, 'MetalGear': 12, 'MetalMax': 38, 'Metro_Cross': 8, 'MickeyMousecapade': 12, 'MikeTyson_sPunch_Out__': 20, 'Millipede': 2, 'Milon_sSecretCastle': 20, 'MonsterinmyPocket': 16, 'Ms_Pac_Man': 4, 'MontyontheRun_MontynoDokiDokiDaisassou': 5, 'MoonCrystal': 20, 'Mother': 40, 'MusashinoKen_TadaimaShugyouChuu': 6, 'NazonoMurasamejou': 9, 'NinjaGaiden': 30, 'NinjaHattori_kun': 9, 'Nobunaga_sAmbition': 9, 'Parodius': 22, 'Nobunaga_sAmbitionII': 16, 'NobunaganoYabou_BushouFuunroku': 25, 'Pac_Mania': 9, 'PhantomFighter': 23, 'PoolofRadiance': 28, 'Pooyan': 18, 'PowerBlade2': 19, 'ProYakyuu_FamilyStadium_88': 13, 'Punch_Out__': 8, 'Puzznic': 8, 'R_B_I_Baseball': 7, 'R_C_Pro_Am': 6, 'RadRacer': 6, 'RadiaSenki_ReimeiHen': 39, 'SkyKid': 4, 'RainbowIslands_TheStoryofBubbleBobble2': 24, 'Renegade': 16, 'RoadFighter': 4, 'RoadRunner': 8, 'RobertByrne_sPoolChallenge': 5, 'RoboWarrior': 14, 'Robocop': 9, 'RollingThunder': 6, 'S_C_A_T__SpecialCyberneticAttackTeam': 13, 'SaintSeiya_OugonDensetsu': 9, 'SaintSeiya_OugonDensetsuKanketsuHen': 12, 'ShadowoftheNinja': 12, 'Shatterhand': 15, 'SidePocket': 9, 'SilverSurfer': 9, 'SolarJetman_HuntfortheGoldenWarpship': 27, 'Solitaire': 4, 'Solomon_sKey': 12, 'Solstice_TheQuestfortheStaffofDemnos': 6, 'Somari': 13, 'SpaceHarrier': 14, 'SpaceHunter': 2, 'SpartanX2': 13, 'Spelunker': 10, 'StarForce': 16, 'StarSoldier': 9, 'StarWars_TheEmpireStrikesBack': 18, 'StarshipHector': 9, 'Stinger': 18, 'Strider': 15, 'SummerCarnival_92_Recca': 13, 'SundayFunday': 2, 'SuperArabian': 4, 'SuperC': 14, 'SuperDonkeyKong': 7, 'SuperGun': 4, 'SuperHIK300in1': 1, 'SuperLodeRunner': 7, 'SuperMarioBros_': 20, 'SuperMarioWorld': 15, 'SuperPitfall': 7, 'SuperStarForce_JikuurekinoHimitsu': 13, 'SuperXevious_GAMPnoNazo': 20, 'SwordMaster': 15, 'TMNetwork_LiveinPowerBowl': 16, 'TaitoChaseH_Q_': 11, 'Tama_amp_Friends_3ChoumeDaibouken': 10, 'Target_Renegade': 10, 'TecmoCupSoccerGame': 29, 'TenchioKurauII_ShokatsuKoumeiDen': 26, 'Tetris': 16, 'TheFlintstones_TheRescueofDino_amp_Hoppy': 12, 'TheFlintstones_TheSurpriseatDinosaurPeak_': 14, 'TheGoonies': 10, 'TheGooniesII': 12, 'TheGuardianLegend': 28, 'TheJungleBook': 12, 'TheKarateKid': 12, 'TheLegendofZelda': 15, 'TheMafatConspiracy': 21, 'ThePandaPrince': 16, 'TheTowerofDruaga': 11, 'Thexder': 1, 'Tiger_Heli': 5, 'Town_amp_CountryII_Thrilla_sSurfari': 9, 'Town_amp_CountrySurfDesigns_Wood_amp_WaterRage': 5, 'Track_amp_Field': 11, 'TwinBee': 9, 'TwinBee3_PokoPokoDaimaou': 19, 'TwinCobra': 10, 'UchuuKeibitaiSDF': 12, 'Ufouria_TheSaga': 13, 'Ultima_Exodus': 11, 'Ultima_QuestoftheAvatar': 30, 'ValkyrienoBouken_TokinoKagiDensetsu': 11, 'VegasDream': 16, 'Vs_BalloonFight': 14, 'Vs_CluCluLand': 8, 'Vs_Excitebike': 22, 'Vs_Hogan_sAlley': 8, 'Vs_RaidonBungelingBay': 3, 'WaiWaiWorld2_SOS__ParsleyJou': 46, 'WaronWheels': 12, 'Warpman': 4, 'Wily_amp_RightnoRockBoard_That_sParadise': 15, 'Wit_s': 19, 'WizardryII_LlylgamynnoIsan': 17, 'Wizardry_KnightofDiamonds_TheSecondScenario': 16, 'Wizardry_ProvingGroundsofTheMadOverlord': 18, 'Wizards_amp_Warriors': 15, 'Xevious': 2, 'YieArKungFu': 3, 'Yo_Noid': 27, 'Yoshi': 13, 'YoukaiDouchuuki': 14, 'YsIII_WanderersfromYs': 27, 'ZombieNation': 22, 'YsII_AncientYsVanishedTheFinalChapter': 27, 'Ys_AncientYsVanishedOmen': 26, 'YuuMaze': 8, 'ZeldaII_TheAdventureofLink': 18, 'ZhongGuoXiangQi': 4, '1943_TheBattleofMidway': 16, 'AlterEgo': 10, 'Arkanoid': 5, 'ArkanoidII': 8, 'BoobyKids': 12, 'BubbleBobble': 8, 'BurgerTime': 4, 'Choplifter': 3, 'DigitalDevilStory_MegamiTensei': 20, 'DragonBusterII_YaminoFuuin': 16, 'DuckHunt': 5, 'FamicomYarou54': 1, 'FleetCommander': 2, 'FormationZ': 2, 'Golgo13_TopSecretEpisode': 24, 'Ghostbusters': 1, 'HikarinoSenshiPhoton_WakuseiZoldiasnoTatakai': 29, 'Hogan_sAlley': 7, 'InsectorX': 13, 'Karnov': 9, 'KnightMove': 9, 'MadoolanoTsubasa': 10, 'MetalFighter_': 3, 'MightyBombJack': 16, 'MiracleRopit_sAdventurein2100': 9, 'MississippiSatsujinJiken': 11, 'OperationWolf': 6, 'PokerJ_ngl_ng': 10, 'Quarth': 7, 'Route16Turbo': 7, 'Rygar': 21, 'Seicross': 7, 'TerraCresta': 11, 'Transformers_ConvoynoNazo': 9, 'Vs_DuckHunt': 10, 'WardnernoMori': 6, 'WreckingCrew': 5, 'AlphaMission': 4, 'Battleship': 9, 'Caesar_sPalace': 4, 'ChukaTaisen': 12, 'DevilWorld': 5, 'DoubleDragonIII_TheSacredStones': 21, 'Exerion': 1, 'Faxanadu': 16, 'Galaxian': 5, 'GuerrillaWar': 15, 'Gyrodine': 2, 'Hu_ngd__Zhu_l_zh_Zh_n': 18, 'Hydlide3_YamiKaranoHoumonsha': 16, 'Ikki': 4, 'IronTank_TheInvasionofNormandy': 8, 'KanshakuTamanageKantarounoTokaidoGojusanTsugi': 14, 'KungFu': 5, 'LodeRunner': 7, 'LittleRedHood': 1, 'LotLot': 4, 'ManiacMansion': 15, 'NintendoWorldChampionships1990': 4, 'OverHorizon': 14, 'P_O_W__PrisonersofWar': 16, 'Pac_Man': 2, 'Penguin_KunWars': 1, 'Pinball': 2, 'Shinobi': 11, 'SkyDestroyer': 2, 'SuperMarioBros_2': 16, 'SuperMarioBros_3': 32, 'TakeshinoChousenjou': 8, 'TimesofLore': 7, 'ToujinMakyouDen_HeraclesnoEikou': 24, 'VS_NinjaJaJaMaru_kun': 14, 'Wayne_sWorld': 8, 'Yoshi_sCookie': 23, 'YumeKoujou_DokiDokiPanic': 15, 'ZeldanoDensetsu': 9, 'ZippyRace': 8}\n",
      "Unique game names: 394\n",
      "Distribution of game counts: {2: 12, 6: 15, 4: 20, 1: 14, 12: 25, 5: 22, 14: 19, 10: 21, 9: 26, 13: 22, 7: 17, 8: 16, 20: 12, 15: 15, 11: 17, 3: 9, 24: 9, 18: 14, 40: 2, 16: 24, 28: 3, 19: 8, 26: 6, 27: 6, 25: 4, 33: 1, 46: 2, 36: 2, 22: 6, 59: 1, 34: 1, 30: 5, 99: 1, 21: 6, 32: 2, 17: 2, 42: 1, 38: 1, 23: 2, 39: 1, 29: 2}\n"
     ]
    }
   ],
   "source": [
    "def extract_game_name(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    game_name_chunks = filename.split('_')[1:-2]\n",
    "    return '_'.join(game_name_chunks)\n",
    "\n",
    "def print_game_distribution(filepaths):\n",
    "    game_names = [extract_game_name(filepath) for filepath in filepaths]\n",
    "    game_name_dist = Counter(game_names)\n",
    "    game_counts = game_name_dist.values()\n",
    "    game_counts_dist = Counter(game_counts)\n",
    "    print(f'Game name distribution: {dict(game_name_dist)}')\n",
    "    print(f'Unique game names: {len(game_name_dist)}')\n",
    "    print(f'Distribution of game counts: {dict(game_counts_dist)}')\n",
    "\n",
    "print_game_distribution(all_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cd1f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original filepaths count: 5244\n",
      "valid conditional filepaths count: 4418\n",
      "number of unique games: 243\n"
     ]
    }
   ],
   "source": [
    "MIN_GAME_COUNT = 10\n",
    "game_names = [extract_game_name(filepath) for filepath in all_filepaths]\n",
    "game_name_dist = Counter(game_names)\n",
    "valid_conditional_filepaths = [filepath for filepath in all_filepaths if game_name_dist[extract_game_name(filepath)] >= MIN_GAME_COUNT]\n",
    "print(\"original filepaths count:\", len(all_filepaths))\n",
    "print(\"valid conditional filepaths count:\", len(valid_conditional_filepaths))\n",
    "print(\"number of unique games:\", sum(1 for count in game_name_dist.values() if count >= MIN_GAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f392a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GAME_NAMES = set([extract_game_name(filepath) for filepath in valid_conditional_filepaths])\n",
    "def get_soloist_conditional_vocabulary():\n",
    "    vocabulary = dict()\n",
    "    index = 0\n",
    "\n",
    "    for special_token in [PAD_TOKEN, BEGINNING_OF_SONG_TOKEN, END_OF_SONG_TOKEN]:\n",
    "        vocabulary[special_token] = index\n",
    "        index += 1\n",
    "\n",
    "    for time_shift in range(1, MAX_SHIFT_STEPS + 1):\n",
    "        vocabulary[f'time_shift_{time_shift}'] = index\n",
    "        index += 1\n",
    "\n",
    "    for action in [\"note_on\", \"note_off\"]:\n",
    "        for pitch in range(128):\n",
    "            vocabulary[f'{action}_{pitch}'] = index\n",
    "            index += 1\n",
    "            \n",
    "    for game_name in VALID_GAME_NAMES:\n",
    "        vocabulary[f'game_{game_name}'] = index\n",
    "        index += 1\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def get_soloist_conditional_id_to_token():\n",
    "    vocabulary = get_soloist_conditional_vocabulary()\n",
    "    return {v: k for k, v in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "837d4017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total soloist conditional vocabulary length 602\n",
      "(game_name, count) sorted:  [('GanbareGoemonGaiden2_TenkanoZaih_', 99), ('FinalFantasyIII', 59), ('DragonWarriorIV', 46), ('WaiWaiWorld2_SOS__ParsleyJou', 46), ('Kirby_sAdventure', 42), ('CaptainTsubasaVol_II_SuperStriker', 40), ('Mother', 40), ('RadiaSenki_ReimeiHen', 39), ('MetalMax', 38), ('FamicomJump_HeroRetsuden', 36)]\n"
     ]
    }
   ],
   "source": [
    "soloist_conditional_vocabulary = get_soloist_conditional_vocabulary()\n",
    "valid_game_name_counts = Counter([extract_game_name(filepath) for filepath in valid_conditional_filepaths])\n",
    "\n",
    "print(\"total soloist conditional vocabulary length\", len(soloist_conditional_vocabulary))\n",
    "print(\"(game_name, count) sorted: \", sorted(valid_game_name_counts.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57b269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_soloist_filepath_to_tokens(filepath, instrument_program, midi_file = None):\n",
    "    game_name = extract_game_name(filepath)\n",
    "    if game_name not in VALID_GAME_NAMES:\n",
    "        raise ValueError(f'Game name {game_name} not in valid game names.')\n",
    "    \n",
    "    pm = midi_file if midi_file else pretty_midi.PrettyMIDI(filepath)\n",
    "    tokens = soloist_midi_to_tokens(pm, instrument_program=instrument_program)\n",
    "    tokens = [tokens[0]] + [f'game_{game_name}'] + tokens[1:] # prefix the sequence with the game name token\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbd6a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens for 005_Abadox_TheDeadlyInnerWar_00_01OpeningSE.mid: ['<BOS>', 'game_Abadox_TheDeadlyInnerWar', 'time_shift_1', 'note_on_45', 'time_shift_1', 'note_off_45', 'time_shift_1', 'note_on_44', 'time_shift_5', 'note_off_44']\n",
      "Example IDs for 005_Abadox_TheDeadlyInnerWar_00_01OpeningSE.mid: [1, 392, 3, 148, 3, 276, 3, 147, 7, 275]\n"
     ]
    }
   ],
   "source": [
    "example_filepath = valid_conditional_filepaths[0]\n",
    "example_tokens = conditional_soloist_filepath_to_tokens(valid_conditional_filepaths[0], instrument_program=80)\n",
    "example_ids = [get_soloist_conditional_vocabulary()[token] for token in example_tokens]\n",
    "print(f'Example tokens for {os.path.basename(example_filepath)}: {example_tokens[:10]}')\n",
    "print(f'Example IDs for {os.path.basename(example_filepath)}: {example_ids[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c782129",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_80 = conditional_soloist_filepath_to_tokens(example_filepath, 80)\n",
    "tokens_81 = conditional_soloist_filepath_to_tokens(example_filepath, 81)\n",
    "tokens_38 = conditional_soloist_filepath_to_tokens(example_filepath, 38)\n",
    "tokens_121 = conditional_soloist_filepath_to_tokens(example_filepath, 121)\n",
    "\n",
    "midi_reconstructed_80 = soloist_tokens_to_midi(tokens_80, instrument_program=80)\n",
    "midi_reconstructed_81 = soloist_tokens_to_midi(tokens_81, instrument_program=81)\n",
    "midi_reconstructed_38 = soloist_tokens_to_midi(tokens_38, instrument_program=38)\n",
    "midi_reconstructed_121 = soloist_tokens_to_midi(tokens_121, instrument_program=121)\n",
    "\n",
    "# midi_reconstructed_80.write('reconstructed_midi_80.mid')\n",
    "# midi_reconstructed_81.write('reconstructed_midi_81.mid')\n",
    "# midi_reconstructed_38.write('reconstructed_midi_38.mid')\n",
    "# midi_reconstructed_121.write('reconstructed_midi_121.mid')\n",
    "\n",
    "# original_midi = pretty_midi.PrettyMIDI(example_filepath)\n",
    "# original_midi.write('original_midi.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74b31afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conditional_soloist_sequences(filepaths, instruments):\n",
    "    vocabulary = get_soloist_conditional_vocabulary()\n",
    "    sequences = {instrument: [] for instrument in instruments}\n",
    "\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        midi_file = pretty_midi.PrettyMIDI(filepath)\n",
    "        for instrument in instruments:\n",
    "            tokens = conditional_soloist_filepath_to_tokens(filepath, instrument, midi_file=midi_file)\n",
    "            if not tokens:\n",
    "                raise ValueError(f'No tokens generated for {filepath} with instrument {instrument}')\n",
    "            sequences[instrument].append([vocabulary[token] for token in tokens])\n",
    "        print_progress_bar(i+1, len(filepaths), prefix='Loading sequences')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "389cea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "all_sequences = load_conditional_soloist_sequences(valid_conditional_filepaths, unique_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f785182",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conditional_80_sequences = all_sequences[80]\n",
    "all_conditional_81_sequences = all_sequences[81]\n",
    "all_conditional_38_sequences = all_sequences[38]\n",
    "all_conditional_121_sequences = all_sequences[121]\n",
    "\n",
    "train_conditional_80_sequences, val_conditional_80_sequences = train_test_split(all_conditional_80_sequences, test_size=0.2, random_state=42)\n",
    "train_conditional_81_sequences, val_conditional_81_sequences = train_test_split(all_conditional_81_sequences, test_size=0.2, random_state=42)\n",
    "train_conditional_38_sequences, val_conditional_38_sequences = train_test_split(all_conditional_38_sequences, test_size=0.2, random_state=42)\n",
    "train_conditional_121_sequences, val_conditional_121_sequences = train_test_split(all_conditional_121_sequences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09270d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_soloist_model():\n",
    "    return MusicRNN(vocab_size=len(get_soloist_conditional_vocabulary()), embedding_dim=64, hidden_dim=512, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "883278aa-235f-406f-8dae-f98d139e0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_soloist_model():\n",
    "    return  MusicTransformer(\n",
    "    vocab_size=len(get_soloist_conditional_vocabulary()),\n",
    "    d_model=256,           # Model dimension (replaces embedding_dim)\n",
    "    nhead=8,               # Number of attention heads\n",
    "    num_layers=2,          # Number of transformer layers\n",
    "    dim_feedforward=2048,  # MLP hidden dimension\n",
    "    dropout=0.1,           # Dropout rate\n",
    "    max_seq_length=512    # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b59dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional_soloists(soloist_sequences: dict):\n",
    "    models = {}\n",
    "    for instrument_program, (train_sequences, val_sequences) in soloist_sequences.items():\n",
    "        train_dataset = MIDITokenDataset(train_sequences, seq_length=512)\n",
    "        val_dataset = MIDITokenDataset(val_sequences, seq_length=512)\n",
    "\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = get_conditional_soloist_model()\n",
    "        train(model, train_loader, val_loader, vocab_size=len(get_soloist_conditional_vocabulary()), device=device)\n",
    "        models[instrument_program] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5252a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 2.3007 | ppl   9.98 | acc 0.479\n",
      "E01 val   | loss 1.7419 | ppl   5.71 | acc 0.538\n",
      "\n",
      "E02 train | loss 1.4319 | ppl   4.19 | acc 0.611\n",
      "E02 val   | loss 1.1517 | ppl   3.16 | acc 0.670\n",
      "\n",
      "E03 train | loss 1.0600 | ppl   2.89 | acc 0.696\n",
      "E03 val   | loss 0.9910 | ppl   2.69 | acc 0.713\n",
      "\n",
      "E04 train | loss 0.9357 | ppl   2.55 | acc 0.730\n",
      "E04 val   | loss 0.9187 | ppl   2.51 | acc 0.737\n",
      "\n",
      "E05 train | loss 0.8685 | ppl   2.38 | acc 0.749\n",
      "E05 val   | loss 0.8842 | ppl   2.42 | acc 0.748\n",
      "\n",
      "E06 train | loss 0.8224 | ppl   2.28 | acc 0.761\n",
      "E06 val   | loss 0.8620 | ppl   2.37 | acc 0.754\n",
      "\n",
      "E07 train | loss 0.7858 | ppl   2.19 | acc 0.771\n",
      "E07 val   | loss 0.8501 | ppl   2.34 | acc 0.757\n",
      "\n",
      "E08 train | loss 0.7560 | ppl   2.13 | acc 0.778\n",
      "E08 val   | loss 0.8385 | ppl   2.31 | acc 0.760\n",
      "\n",
      "E09 train | loss 0.7302 | ppl   2.08 | acc 0.785\n",
      "E09 val   | loss 0.8342 | ppl   2.30 | acc 0.762\n",
      "\n",
      "E10 train | loss 0.7091 | ppl   2.03 | acc 0.790\n",
      "E10 val   | loss 0.8353 | ppl   2.31 | acc 0.762\n",
      "\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 2.4542 | ppl  11.64 | acc 0.461\n",
      "E01 val   | loss 1.8189 | ppl   6.17 | acc 0.517\n",
      "\n",
      "E02 train | loss 1.5358 | ppl   4.65 | acc 0.587\n",
      "E02 val   | loss 1.2530 | ppl   3.50 | acc 0.645\n",
      "\n",
      "E03 train | loss 1.1201 | ppl   3.07 | acc 0.681\n",
      "E03 val   | loss 1.0657 | ppl   2.90 | acc 0.691\n",
      "\n",
      "E04 train | loss 0.9712 | ppl   2.64 | acc 0.720\n",
      "E04 val   | loss 0.9584 | ppl   2.61 | acc 0.722\n",
      "\n",
      "E05 train | loss 0.8861 | ppl   2.43 | acc 0.743\n",
      "E05 val   | loss 0.9157 | ppl   2.50 | acc 0.736\n",
      "\n",
      "E06 train | loss 0.8345 | ppl   2.30 | acc 0.758\n",
      "E06 val   | loss 0.8838 | ppl   2.42 | acc 0.744\n",
      "\n",
      "E07 train | loss 0.7981 | ppl   2.22 | acc 0.767\n",
      "E07 val   | loss 0.8705 | ppl   2.39 | acc 0.748\n",
      "\n",
      "E08 train | loss 0.7694 | ppl   2.16 | acc 0.775\n",
      "E08 val   | loss 0.8575 | ppl   2.36 | acc 0.752\n",
      "\n",
      "E09 train | loss 0.7453 | ppl   2.11 | acc 0.781\n",
      "E09 val   | loss 0.8548 | ppl   2.35 | acc 0.753\n",
      "\n",
      "E10 train | loss 0.7236 | ppl   2.06 | acc 0.786\n",
      "E10 val   | loss 0.8487 | ppl   2.34 | acc 0.755\n",
      "\n",
      "Processing sequences |█████████████████████████████████████████---------| 83.8% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 2.3455 | ppl  10.44 | acc 0.458\n",
      "E01 val   | loss 1.8152 | ppl   6.14 | acc 0.513\n",
      "\n",
      "E02 train | loss 1.4387 | ppl   4.22 | acc 0.598\n",
      "E02 val   | loss 1.2063 | ppl   3.34 | acc 0.657\n",
      "\n",
      "E03 train | loss 1.0170 | ppl   2.76 | acc 0.705\n",
      "E03 val   | loss 0.9769 | ppl   2.66 | acc 0.717\n",
      "\n",
      "E04 train | loss 0.8740 | ppl   2.40 | acc 0.745\n",
      "E04 val   | loss 0.9301 | ppl   2.53 | acc 0.731\n",
      "\n",
      "E05 train | loss 0.8121 | ppl   2.25 | acc 0.760\n",
      "E05 val   | loss 0.9081 | ppl   2.48 | acc 0.736\n",
      "\n",
      "E06 train | loss 0.7693 | ppl   2.16 | acc 0.771\n",
      "E06 val   | loss 0.8913 | ppl   2.44 | acc 0.740\n",
      "\n",
      "E07 train | loss 0.7365 | ppl   2.09 | acc 0.780\n",
      "E07 val   | loss 0.8841 | ppl   2.42 | acc 0.743\n",
      "\n",
      "E08 train | loss 0.7088 | ppl   2.03 | acc 0.787\n",
      "E08 val   | loss 0.8783 | ppl   2.41 | acc 0.744\n",
      "\n",
      "E09 train | loss 0.6853 | ppl   1.98 | acc 0.794\n",
      "E09 val   | loss 0.8784 | ppl   2.41 | acc 0.747\n",
      "\n",
      "E10 train | loss 0.6629 | ppl   1.94 | acc 0.800\n",
      "E10 val   | loss 0.8877 | ppl   2.43 | acc 0.746\n",
      "\n",
      "Processing sequences |██████████████████████████████████████████--------| 84.4% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences |██████████████████████████████████████████████████| 100.0% Complete\n",
      "E01 train | loss 1.7553 | ppl   5.79 | acc 0.507\n",
      "E01 val   | loss 1.2424 | ppl   3.46 | acc 0.581\n",
      "\n",
      "E02 train | loss 1.0591 | ppl   2.88 | acc 0.633\n",
      "E02 val   | loss 0.8802 | ppl   2.41 | acc 0.678\n",
      "\n",
      "E03 train | loss 0.7983 | ppl   2.22 | acc 0.714\n",
      "E03 val   | loss 0.6520 | ppl   1.92 | acc 0.770\n",
      "\n",
      "E04 train | loss 0.5808 | ppl   1.79 | acc 0.799\n",
      "E04 val   | loss 0.4766 | ppl   1.61 | acc 0.842\n",
      "\n",
      "E05 train | loss 0.4637 | ppl   1.59 | acc 0.842\n",
      "E05 val   | loss 0.4171 | ppl   1.52 | acc 0.863\n",
      "\n",
      "E06 train | loss 0.4079 | ppl   1.50 | acc 0.861\n",
      "E06 val   | loss 0.3980 | ppl   1.49 | acc 0.868\n",
      "\n",
      "E07 train | loss 0.3772 | ppl   1.46 | acc 0.871\n",
      "E07 val   | loss 0.3906 | ppl   1.48 | acc 0.870\n",
      "\n",
      "E08 train | loss 0.3550 | ppl   1.43 | acc 0.878\n",
      "E08 val   | loss 0.3811 | ppl   1.46 | acc 0.872\n",
      "\n",
      "E09 train | loss 0.3428 | ppl   1.41 | acc 0.881\n",
      "E09 val   | loss 0.3815 | ppl   1.46 | acc 0.873\n",
      "\n",
      "E10 train | loss 0.3334 | ppl   1.40 | acc 0.884\n",
      "E10 val   | loss 0.3847 | ppl   1.47 | acc 0.874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soloist_sequences = {\n",
    "    80: (train_conditional_80_sequences, val_conditional_80_sequences),\n",
    "    81: (train_conditional_81_sequences, val_conditional_81_sequences),\n",
    "    38: (train_conditional_38_sequences, val_conditional_38_sequences),\n",
    "    121: (train_conditional_121_sequences, val_conditional_121_sequences)\n",
    "}\n",
    "soloist_models = train_conditional_soloists(soloist_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5596026-9597-416b-9ec6-2a00b4ef9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports ---------------------------------------------------------------\n",
    "import os, datetime, torch, torch.nn.functional as F, pretty_midi\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# 1.  Conditional autoregressive sampling for a MusicTransformer\n",
    "# -------------------------------------------------------------------------- #\n",
    "def sample_conditional_transformer(\n",
    "    model: torch.nn.Module,\n",
    "    prefix_tokens: List[int],                 # e.g. [BOS, game_token]\n",
    "    vocab: Dict[str, int],\n",
    "    max_length: int = 512,                    # keep ≤ pos-embed size\n",
    "    temperature: float = 1.0,\n",
    "    device: str = \"cuda\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate a continuation given a fixed token prefix (no gradient).\n",
    "\n",
    "    Returns the full sequence: prefix + generated suffix.\n",
    "    \"\"\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # Fast sanity-check: do we exceed the learned positional table?\n",
    "    seq_cap = getattr(model, \"max_seq_length\", model.pos_embedding.num_embeddings)\n",
    "    if max_length > seq_cap:\n",
    "        max_length = seq_cap                    # hard-clip to avoid CUDA asserts\n",
    "\n",
    "    eos_id, pad_id = vocab[END_OF_SONG_TOKEN], vocab[PAD_TOKEN]\n",
    "\n",
    "    generated = list(prefix_tokens)\n",
    "    tokens = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(prefix_tokens)):\n",
    "            logits, _ = model(tokens)           # (1, T, |V|)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "            next_tok = torch.multinomial(probs, 1).item()\n",
    "            generated.append(next_tok)\n",
    "\n",
    "            if next_tok in (eos_id, pad_id):\n",
    "                break\n",
    "\n",
    "            tokens = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "# 2.  Convenience wrapper: generate every solo line and merge to one MIDI\n",
    "# -------------------------------------------------------------------------- #\n",
    "def sample_conditional_soloists(\n",
    "    models: Dict[int, torch.nn.Module],\n",
    "    game_name: str,\n",
    "    outdir: str = \"./\",\n",
    "    max_length: int = 512,\n",
    "    temperature: float = 1.0,\n",
    "    device: str | None = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    For each instrument program in `models`, generate a transformer-based\n",
    "    conditional solo line (conditioned on `game_name`) and write a merged MIDI.\n",
    "\n",
    "    Returns the path of the generated file.\n",
    "    \"\"\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    vocab         = get_soloist_conditional_vocabulary()\n",
    "    id_to_token   = get_soloist_conditional_id_to_token()\n",
    "    game_tok_key  = f\"game_{game_name}\"\n",
    "    if game_tok_key not in vocab:\n",
    "        raise KeyError(f\"'{game_tok_key}' not found in conditional vocabulary\")\n",
    "\n",
    "    prefix = [vocab[BEGINNING_OF_SONG_TOKEN], vocab[game_tok_key]]\n",
    "\n",
    "    midis = OrderedDict()\n",
    "    for prog, model in models.items():\n",
    "        tokens_ids = sample_conditional_transformer(\n",
    "            model,\n",
    "            prefix_tokens=prefix,\n",
    "            vocab=vocab,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            device=device,\n",
    "        )\n",
    "        tokens_txt = [id_to_token[i] for i in tokens_ids]\n",
    "        midi_track = soloist_tokens_to_midi(tokens_txt, instrument_program=prog)\n",
    "        midis[prog] = midi_track\n",
    "\n",
    "    # Merge the generated instrument tracks\n",
    "    merged = pretty_midi.PrettyMIDI()\n",
    "    for midi in midis.values():\n",
    "        merged.instruments.extend(midi.instruments)\n",
    "\n",
    "    outdir = Path(outdir) / game_name\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    outpath = outdir / f\"{int(datetime.datetime.now().timestamp()*1000)}.mid\"\n",
    "    merged.write(str(outpath))     # or outpath.as_posix()\n",
    "\n",
    "\n",
    "    print(f\"🎮  Conditional soloists written to {outpath}\")\n",
    "    return outpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5d3298f-934e-416e-9a28-e4e4d1d10316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping note_on_47, other note: 75 is still active.\n",
      "Warning: Note off for 47 without matching note on.\n",
      "Warning: Skipping note_on_52, other note: 75 is still active.\n",
      "Warning: Note off for 52 without matching note on.\n",
      "Warning: Skipping note_on_51, other note: 75 is still active.\n",
      "Warning: Note off for 51 without matching note on.\n",
      "Warning: Skipping note_on_59, other note: 75 is still active.\n",
      "Warning: Note off for 59 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 75 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_61, other note: 75 is still active.\n",
      "Warning: Note off for 61 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 75 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_58, other note: 75 is still active.\n",
      "Warning: Note off for 58 without matching note on.\n",
      "Warning: Skipping note_on_58, other note: 75 is still active.\n",
      "Warning: Note off for 58 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 75 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 75 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_59, other note: 75 is still active.\n",
      "Warning: Note off for 59 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 75 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 75 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_59, other note: 75 is still active.\n",
      "Warning: Note off for 59 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 75 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 75 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 75 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 75 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 75 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 60 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_41, other note: 60 is still active.\n",
      "Warning: Note off for 41 without matching note on.\n",
      "Warning: Skipping note_on_60, other note: 60 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_60, other note: 62 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_60, other note: 62 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 62 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 62 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 62 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_60, other note: 62 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_60, other note: 62 is still active.\n",
      "Warning: Note off for 60 without matching note on.\n",
      "Warning: Skipping note_on_64, other note: 62 is still active.\n",
      "Warning: Note off for 64 without matching note on.\n",
      "Warning: Skipping note_on_64, other note: 62 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 62 is still active.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_65, other note: 62 is still active.\n",
      "Warning: Note off for 65 without matching note on.\n",
      "Warning: Skipping note_on_93, other note: 62 is still active.\n",
      "Warning: Note off for 93 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_59, other note: 62 is still active.\n",
      "Warning: Note off for 59 without matching note on.\n",
      "Warning: Skipping note_on_59, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 62 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Note off for 68 without matching note on.\n",
      "🎮  Conditional soloists written to transformer_models/conditioned_soloists/1/samples/GanbareGoemonGaiden2_TenkanoZaih_/1748910911958.mid\n",
      "Warning: Skipping note_on_76, other note: 78 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 78 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 78 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 74 is still active.\n",
      "Warning: Note off for 78 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 74 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 74 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 74 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 74 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 72 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 72 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_83, other note: 72 is still active.\n",
      "Warning: Note off for 83 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 72 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 72 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 72 is still active.\n",
      "Warning: Note off for 78 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 72 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 72 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_73, other note: 72 is still active.\n",
      "Warning: Note off for 73 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 72 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 72 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 72 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_68, other note: 72 is still active.\n",
      "Warning: Note off for 68 without matching note on.\n",
      "Warning: Skipping note_on_58, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 72 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 72 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 72 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 72 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 72 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 72 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 72 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 72 is still active.\n",
      "Warning: Note off for 78 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 72 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 72 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 72 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 72 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 72 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 72 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 72 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 72 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_83, other note: 72 is still active.\n",
      "Warning: Note off for 83 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 72 is still active.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_46, other note: 62 is still active.\n",
      "Warning: Note off for 46 without matching note on.\n",
      "Warning: Skipping note_on_54, other note: 62 is still active.\n",
      "Warning: Note off for 54 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_51, other note: 62 is still active.\n",
      "Warning: Note off for 51 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_73, other note: 62 is still active.\n",
      "Warning: Note off for 73 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 62 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_39, other note: 62 is still active.\n",
      "Warning: Note off for 39 without matching note on.\n",
      "Warning: Skipping note_on_40, other note: 62 is still active.\n",
      "Warning: Note off for 40 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 62 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 62 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 62 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_91, other note: 62 is still active.\n",
      "Warning: Note off for 91 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_80, other note: 62 is still active.\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_88, other note: 62 is still active.\n",
      "Warning: Note off for 88 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_91, other note: 62 is still active.\n",
      "Warning: Note off for 91 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 62 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 62 is still active.\n",
      "Warning: Note off for 78 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 62 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 62 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 62 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 62 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_91, other note: 62 is still active.\n",
      "Warning: Note off for 91 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_88, other note: 62 is still active.\n",
      "Warning: Skipping note_on_63, other note: 62 is still active.\n",
      "Warning: Note off for 63 without matching note on.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 62 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_50, other note: 62 is still active.\n",
      "Warning: Note off for 50 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 62 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_98, other note: 62 is still active.\n",
      "Warning: Skipping note_on_57, other note: 62 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_68, other note: 62 is still active.\n",
      "Warning: Note off for 68 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 62 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_107, other note: 62 is still active.\n",
      "Warning: Note off for 107 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 62 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 62 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 62 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_87, other note: 62 is still active.\n",
      "Warning: Note off for 87 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 62 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_85, other note: 62 is still active.\n",
      "Warning: Note off for 85 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 62 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_73, other note: 62 is still active.\n",
      "Warning: Note off for 73 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_63, other note: 62 is still active.\n",
      "Warning: Note off for 63 without matching note on.\n",
      "Warning: Skipping note_on_68, other note: 62 is still active.\n",
      "Warning: Note off for 68 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 62 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 62 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 62 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 62 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Skipping note_on_58, other note: 62 is still active.\n",
      "Warning: Note off for 58 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 62 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_107, other note: 62 is still active.\n",
      "Warning: Note off for 107 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 62 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 62 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_74, other note: 62 is still active.\n",
      "Warning: Note off for 74 without matching note on.\n",
      "Warning: Skipping note_on_72, other note: 62 is still active.\n",
      "Warning: Note off for 72 without matching note on.\n",
      "Warning: Note off for 52 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 62 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_50, other note: 81 is still active.\n",
      "Warning: Note off for 50 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_42, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_41, other note: 81 is still active.\n",
      "Warning: Note off for 45 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 81 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_54, other note: 81 is still active.\n",
      "Warning: Note off for 54 without matching note on.\n",
      "Warning: Skipping note_on_50, other note: 81 is still active.\n",
      "Warning: Note off for 50 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 50 without matching note on.\n",
      "Warning: Skipping note_on_53, other note: 81 is still active.\n",
      "Warning: Note off for 53 without matching note on.\n",
      "Warning: Skipping note_on_42, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_42, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_41, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_53, other note: 81 is still active.\n",
      "Warning: Note off for 53 without matching note on.\n",
      "Warning: Skipping note_on_50, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_55, other note: 81 is still active.\n",
      "Warning: Note off for 55 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_43, other note: 81 is still active.\n",
      "Warning: Note off for 43 without matching note on.\n",
      "Warning: Skipping note_on_42, other note: 81 is still active.\n",
      "Warning: Note off for 42 without matching note on.\n",
      "Warning: Skipping note_on_38, other note: 81 is still active.\n",
      "Warning: Note off for 38 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 9 is still active.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 9 is still active.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 9 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 9 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 9 is still active.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 9 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 9 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 9 is still active.\n",
      "Warning: Skipping note_on_15, other note: 14 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 14 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_16, other note: 8 is still active.\n",
      "Warning: Note off for 16 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_8, other note: 8 is still active.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Skipping note_on_8, other note: 8 is still active.\n",
      "Warning: Note off for 11 without matching note on.\n",
      "Warning: Skipping note_on_11, other note: 8 is still active.\n",
      "Warning: Note off for 11 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_4, other note: 8 is still active.\n",
      "Warning: Note off for 4 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_9, other note: 8 is still active.\n",
      "Warning: Note off for 9 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_15, other note: 8 is still active.\n",
      "Warning: Note off for 15 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_14, other note: 8 is still active.\n",
      "Warning: Note off for 14 without matching note on.\n",
      "Warning: Skipping note_on_8, other note: 8 is still active.\n",
      "🎮  Conditional soloists written to transformer_models/conditioned_soloists/1/samples/FinalFantasyIII/1748910932632.mid\n",
      "Warning: Skipping note_on_65, other note: 65 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 65 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_86, other note: 65 is still active.\n",
      "Warning: Note off for 86 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 65 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_78, other note: 65 is still active.\n",
      "Warning: Note off for 78 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 65 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 65 is still active.\n",
      "Warning: Note off for 77 without matching note on.\n",
      "Warning: Skipping note_on_79, other note: 65 is still active.\n",
      "Warning: Note off for 79 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 65 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 65 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_73, other note: 65 is still active.\n",
      "Warning: Note off for 73 without matching note on.\n",
      "Warning: Skipping note_on_76, other note: 65 is still active.\n",
      "Warning: Note off for 76 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 65 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_71, other note: 65 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 65 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_58, other note: 65 is still active.\n",
      "Warning: Note off for 58 without matching note on.\n",
      "Warning: Skipping note_on_64, other note: 65 is still active.\n",
      "Warning: Note off for 64 without matching note on.\n",
      "Warning: Skipping note_on_54, other note: 65 is still active.\n",
      "Warning: Note off for 54 without matching note on.\n",
      "Warning: Skipping note_on_48, other note: 65 is still active.\n",
      "Warning: Note off for 48 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 65 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_88, other note: 65 is still active.\n",
      "Warning: Note off for 88 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 65 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 65 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 65 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_61, other note: 65 is still active.\n",
      "Warning: Note off for 71 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 65 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_69, other note: 65 is still active.\n",
      "Warning: Note off for 69 without matching note on.\n",
      "Warning: Skipping note_on_73, other note: 65 is still active.\n",
      "Warning: Note off for 73 without matching note on.\n",
      "Warning: Skipping note_on_70, other note: 65 is still active.\n",
      "Warning: Note off for 70 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_50, other note: 65 is still active.\n",
      "Warning: Note off for 50 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 65 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 65 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_98, other note: 65 is still active.\n",
      "Warning: Note off for 98 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 65 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_98, other note: 65 is still active.\n",
      "Warning: Note off for 98 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 65 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_62, other note: 65 is still active.\n",
      "Warning: Note off for 62 without matching note on.\n",
      "Warning: Skipping note_on_57, other note: 65 is still active.\n",
      "Warning: Note off for 57 without matching note on.\n",
      "Warning: Skipping note_on_61, other note: 65 is still active.\n",
      "Warning: Note off for 61 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 65 is still active.\n",
      "Warning: Note off for 67 without matching note on.\n",
      "Warning: Skipping note_on_64, other note: 65 is still active.\n",
      "Warning: Note off for 64 without matching note on.\n",
      "Warning: Skipping note_on_64, other note: 65 is still active.\n",
      "Warning: Note off for 64 without matching note on.\n",
      "Warning: Skipping note_on_66, other note: 65 is still active.\n",
      "Warning: Note off for 66 without matching note on.\n",
      "Warning: Skipping note_on_63, other note: 65 is still active.\n",
      "Warning: Note off for 63 without matching note on.\n",
      "Warning: Skipping note_on_61, other note: 65 is still active.\n",
      "Warning: Note off for 61 without matching note on.\n",
      "Warning: Skipping note_on_52, other note: 65 is still active.\n",
      "Warning: Note off for 52 without matching note on.\n",
      "Warning: Skipping note_on_68, other note: 65 is still active.\n",
      "Warning: Note off for 68 without matching note on.\n",
      "Warning: Skipping note_on_67, other note: 65 is still active.\n",
      "🎮  Conditional soloists written to transformer_models/conditioned_soloists/1/samples/WaiWaiWorld2_SOS__ParsleyJou/1748910949039.mid\n",
      "Warning: Note off for 80 without matching note on.\n",
      "Warning: Skipping note_on_75, other note: 77 is still active.\n",
      "Warning: Note off for 75 without matching note on.\n",
      "Warning: Skipping note_on_84, other note: 77 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "Warning: Skipping note_on_82, other note: 77 is still active.\n",
      "Warning: Note off for 82 without matching note on.\n",
      "Warning: Skipping note_on_81, other note: 77 is still active.\n",
      "Warning: Note off for 81 without matching note on.\n",
      "Warning: Skipping note_on_77, other note: 77 is still active.\n",
      "Warning: Note off for 84 without matching note on.\n",
      "🎮  Conditional soloists written to transformer_models/conditioned_soloists/1/samples/DragonWarriorIV/1748910964348.mid\n",
      "🎮  Conditional soloists written to transformer_models/conditioned_soloists/1/samples/Kirby_sAdventure/1748910982547.mid\n"
     ]
    }
   ],
   "source": [
    "games_to_sample = [\n",
    "    \"GanbareGoemonGaiden2_TenkanoZaih_\", \"FinalFantasyIII\",\n",
    "    \"WaiWaiWorld2_SOS__ParsleyJou\", \"DragonWarriorIV\", \"Kirby_sAdventure\"\n",
    "]\n",
    "\n",
    "weights_dir = Path(\"transformer_models\") / \"conditioned_soloists\" / \"1\"\n",
    "# conditional_models = {\n",
    "#     prog: load_model_weight(\n",
    "#         weights_dir / f\"soloist_conditional_model_{prog}.pth\",\n",
    "#         get_conditional_soloist_transformer()  # <-- transformer factory\n",
    "#     )\n",
    "#     for prog in unique_instruments\n",
    "# }\n",
    "\n",
    "for game in games_to_sample:\n",
    "    sample_conditional_soloists(\n",
    "        soloist_models,\n",
    "        game,\n",
    "        outdir=weights_dir / \"samples\",\n",
    "        max_length=512,         # must be ≤ pos_embed size\n",
    "        temperature=1.1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional(model, start_tokens, max_length=100, temperature=1.0, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    generated = start_tokens\n",
    "    input_token = torch.tensor([generated], device=device)  # (1, 1)\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input_token, hidden)  # output: (1, 1, vocab_size)\n",
    "        output = output[:, -1, :]  # take the last output\n",
    "        output = output / temperature  # adjust randomness\n",
    "\n",
    "        probs = F.softmax(output, dim=-1)  # (1, vocab_size)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == VOCABULARY[END_OF_SONG_TOKEN] or VOCABULARY[PAD_TOKEN]: # reach end of sequence\n",
    "          break\n",
    "\n",
    "        input_token = torch.tensor([[next_token]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f380e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional_soloists(models, game_name, outdir='./'):\n",
    "    soloist_vocabulary = get_soloist_conditional_vocabulary()\n",
    "    midis = dict()\n",
    "    for instrument_program, model in models.items():\n",
    "        model.eval()  # Set to eval mode for inference\n",
    "        game_token = f'game_{game_name}'\n",
    "        start_tokens = [soloist_vocabulary[BEGINNING_OF_SONG_TOKEN], soloist_vocabulary[game_token]]\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        generated_sequence = sample_conditional(model, start_tokens, max_length=1024, device=device)\n",
    "        generated_tokens = [get_soloist_conditional_id_to_token()[token] for token in generated_sequence]\n",
    "        tokens_reconstructed = soloist_tokens_to_midi(generated_tokens, instrument_program=instrument_program)\n",
    "        midis[instrument_program] = tokens_reconstructed\n",
    "    \n",
    "    together_midi = pretty_midi.PrettyMIDI()\n",
    "    for instrument_program, midi in midis.items():\n",
    "        together_midi.instruments.extend(midi.instruments)\n",
    "\n",
    "    outdir = os.path.join(outdir, game_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = os.path.join(outdir, f'{int(datetime.datetime.now().timestamp() * 1000)}.mid')\n",
    "    \n",
    "    together_midi.write(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ec699",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_to_sample = ['GanbareGoemonGaiden2_TenkanoZaih_', 'FinalFantasyIII', 'WaiWaiWorld2_SOS__ParsleyJou', 'DragonWarriorIV', 'Kirby_sAdventure']\n",
    "model_weights_dir = os.path.join('lstm_models', 'conditioned_soloists', '1')\n",
    "conditioned_models = {instr_program: load_model_weight(os.path.join(model_weights_dir, f'soloist_conditional_model_{instr_program}.pth'), get_conditional_soloist_model()) for instr_program in unique_instruments}\n",
    "\n",
    "for game in games_to_sample:\n",
    "    sample_conditional_soloists(conditioned_models, game, outdir=os.path.join(model_weights_dir, 'samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323e509",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c085537",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_dirpath = 'nesmdb_midi/'\n",
    "midi_train_dirpath = os.path.join(midi_dirpath, 'train')\n",
    "midi_test_dirpath = os.path.join(midi_dirpath, 'test')\n",
    "midi_val_dirpath = os.path.join(midi_dirpath, 'valid')\n",
    "midi_train_filesnames = os.listdir(midi_train_dirpath)\n",
    "midi_test_filesnames = os.listdir(midi_test_dirpath)\n",
    "midi_val_filenames = os.listdir(midi_val_dirpath)\n",
    "\n",
    "midi_train_filepaths = [os.path.join(midi_train_dirpath, filename) for filename in midi_train_filesnames]\n",
    "midi_test_filepaths = [os.path.join(midi_test_dirpath, filename) for filename in midi_test_filesnames]\n",
    "midi_val_filepaths = [os.path.join(midi_val_dirpath, filename) for filename in midi_val_filenames]\n",
    "all_filepaths = midi_train_filepaths + midi_test_filepaths + midi_val_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "program_counts = Counter()\n",
    "num_programs_counts = Counter()\n",
    "\n",
    "data = []\n",
    "\n",
    "unique_programs = set()\n",
    "\n",
    "note_counts = Counter()\n",
    "num_notes_counts = Counter()\n",
    "for i, filepath in enumerate(all_filepaths):\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(filepath)\n",
    "\n",
    "        length = midi.get_end_time()\n",
    "        instruments = midi.instruments\n",
    "        num_instruments = len(instruments)\n",
    "\n",
    "\n",
    "        num_notes = sum(len(instr.notes) for instr in instruments)\n",
    "        num_notes_counts[num_notes] += 1\n",
    "        \n",
    "\n",
    "        for instrument in instruments:\n",
    "            program_counts[instrument.program] += 1\n",
    "            unique_programs.add(instrument.program)\n",
    "\n",
    "            for note in instrument.notes:\n",
    "                note_counts[note.pitch] += 1\n",
    "                \n",
    "        num_programs_counts[len(instruments)] += 1\n",
    "    \n",
    "        data.append({\n",
    "            \"length_sec\": length,\n",
    "            \"over_10_sec\": length > 10,\n",
    "            \"note_count\": num_notes\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "    print_progress_bar(i+1, len(all_filepaths), prefix='Loading all MIDI files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d92040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "all_instruments = []\n",
    "for number_of_instruments, count in num_programs_counts.items():\n",
    "    all_instruments += [number_of_instruments] * count\n",
    "\n",
    "summary = {\n",
    "    \"Total MIDI files\": len(df),\n",
    "    \"Total number of notes\": df[\"note_count\"].sum(),\n",
    "    \"Total length (hours)\": round(df[\"length_sec\"].sum()/60/60, 1),\n",
    "    \"Average file duration (sec)\": round(df[\"length_sec\"].mean(), 1),\n",
    "    \"MIDI files > 10s\": df[\"over_10_sec\"].sum(),\n",
    "    \"MIDI files > 45s\": df[df[\"length_sec\"] > 45].shape[0],\n",
    "    \"Unique instruments\": len(unique_programs),\n",
    "    \"Average number of instruments per file\": round(np.mean(all_instruments), 1),\n",
    "    }\n",
    "pd.DataFrame.from_dict(summary, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = sorted(num_programs_counts.keys())\n",
    "y = [num_programs_counts[k] for k in x]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Distribution of Number of Instruments per MIDI File\")\n",
    "plt.xlabel(\"Number of Instruments\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(x)  # Show each instrument count as a tick\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ede62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Convert to instrument names\n",
    "from pretty_midi import program_to_instrument_name\n",
    "\n",
    "program_names = {\n",
    "    program_to_instrument_name(p): c for p, c in program_counts.items()\n",
    "}\n",
    "df_instr = pd.DataFrame(program_names.items(), columns=[\"Instrument\", \"Count\"]).sort_values(\"Count\", ascending=False)\n",
    "\n",
    "sns.barplot(data=df_instr, x=\"Count\", y=\"Instrument\")\n",
    "plt.title(\"Occurrences of Each Instrument\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Instrument\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86649218",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(df[\"length_sec\"], bins=50, ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of MIDI Lengths\")\n",
    "axs[0].set_xlabel(\"Length (seconds)\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.histplot(df[\"note_count\"], bins=50, ax=axs[1])\n",
    "axs[1].set_title(\"Distribution of Note Counts\")\n",
    "axs[1].set_xlabel(\"Note Count\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd81398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "all_instruments = []\n",
    "for number_of_instruments, count in num_programs_counts.items():\n",
    "    all_instruments += [number_of_instruments] * count\n",
    "\n",
    "MIN_GAME_COUNT = 10\n",
    "game_names = [extract_game_name(filepath) for filepath in all_filepaths]\n",
    "game_name_dist = Counter(game_names)\n",
    "valid_conditional_filepaths = [filepath for filepath in all_filepaths if game_name_dist[extract_game_name(filepath)] >= MIN_GAME_COUNT]\n",
    "valid_game_names = set([extract_game_name(filepath) for filepath in valid_conditional_filepaths])\n",
    "\n",
    "summary = {\n",
    "    \"Total MIDI files\": len(df),\n",
    "    \"Total number of notes\": df[\"note_count\"].sum(),\n",
    "    \"Total length (hours)\": round(df[\"length_sec\"].sum()/60/60, 1),\n",
    "    \"Average file duration (sec)\": round(df[\"length_sec\"].mean(), 1),\n",
    "    \"MIDI files > 10s\": df[\"over_10_sec\"].sum(),\n",
    "    \"MIDI files > 45s\": df[df[\"length_sec\"] > 45].shape[0],\n",
    "    \"Unique instruments\": len(unique_programs),\n",
    "    \"Average number of instruments per file\": round(np.mean(all_instruments), 1),\n",
    "    \"Number of games\": len(set(game_names)),\n",
    "    \"Games with at least 10 files\": len(set(valid_game_names)),\n",
    "    }\n",
    "pd.DataFrame.from_dict(summary, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "game_name_counts_dist = Counter(game_name_dist.values())\n",
    "x = sorted(game_name_counts_dist.keys())  # x: number of files per game\n",
    "y = [game_name_counts_dist[k] for k in x]  # y: how many games have that many files\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Distribution of Number of Files per Game\")\n",
    "plt.xlabel(\"Number of Files\")\n",
    "plt.ylabel(\"Games with This Many Files\")\n",
    "plt.xticks(x[::2])\n",
    "plt.tight_layout()\n",
    "plt.xlim(right=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf266891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
